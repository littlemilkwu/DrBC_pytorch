{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import read_test1_data\n",
    "from utils import gen_graph\n",
    "from utils import prepare_synthetic\n",
    "from utils import shuffle_graph\n",
    "from utils import preprocessing_data\n",
    "from utils import get_pairwise_ids\n",
    "\n",
    "from utils import prepare_test1\n",
    "from utils import top_n_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 11\n",
    "SYNTHETIC_NUM = 50\n",
    "# SYNTHETIC_NUM = 1000\n",
    "\n",
    "# number of gen nodes\n",
    "# NUM_MIN = 4000\n",
    "# NUM_MAX = 4001\n",
    "NUM_MIN = 200\n",
    "NUM_MAX = 201\n",
    "\n",
    "\n",
    "MAX_EPOCHS = 10000\n",
    "LEARNING_RATE = 1e-4\n",
    "EMBEDDING_SIZE = 128\n",
    "DEPTH = 5\n",
    "BATCH_SIZE = 16\n",
    "# BATCH_SIZE = 1\n",
    "\n",
    "TEST1_NUM = 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1_g, test1_bc, test1_edgeindex = read_test1_data(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Synthetic Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1984\n"
     ]
    }
   ],
   "source": [
    "train_g = gen_graph(500, 501)\n",
    "print(len(train_g.edges()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DegreeView({0: 48, 1: 42, 2: 2, 3: 41, 4: 70, 5: 44, 6: 74, 7: 40, 8: 37, 9: 30, 10: 23, 11: 20, 12: 22, 13: 25, 14: 13, 15: 21, 16: 46, 17: 44, 18: 18, 19: 25, 20: 19, 21: 25, 22: 45, 23: 14, 24: 30, 25: 22, 26: 19, 27: 19, 28: 24, 29: 20, 30: 19, 31: 19, 32: 9, 33: 10, 34: 10, 35: 6, 36: 7, 37: 22, 38: 10, 39: 8, 40: 11, 41: 12, 42: 13, 43: 19, 44: 7, 45: 11, 46: 17, 47: 7, 48: 9, 49: 12, 50: 14, 51: 12, 52: 11, 53: 14, 54: 6, 55: 12, 56: 9, 57: 9, 58: 13, 59: 6, 60: 13, 61: 7, 62: 20, 63: 7, 64: 17, 65: 20, 66: 11, 67: 8, 68: 10, 69: 11, 70: 15, 71: 15, 72: 13, 73: 21, 74: 18, 75: 9, 76: 11, 77: 9, 78: 6, 79: 9, 80: 10, 81: 16, 82: 22, 83: 6, 84: 16, 85: 6, 86: 8, 87: 7, 88: 19, 89: 17, 90: 10, 91: 12, 92: 10, 93: 6, 94: 16, 95: 11, 96: 8, 97: 8, 98: 12, 99: 9, 100: 5, 101: 6, 102: 4, 103: 8, 104: 8, 105: 14, 106: 6, 107: 5, 108: 8, 109: 4, 110: 6, 111: 8, 112: 8, 113: 7, 114: 5, 115: 14, 116: 15, 117: 16, 118: 6, 119: 10, 120: 5, 121: 9, 122: 5, 123: 8, 124: 7, 125: 9, 126: 5, 127: 6, 128: 9, 129: 10, 130: 8, 131: 5, 132: 6, 133: 7, 134: 7, 135: 9, 136: 15, 137: 8, 138: 6, 139: 11, 140: 11, 141: 8, 142: 6, 143: 8, 144: 8, 145: 7, 146: 13, 147: 8, 148: 7, 149: 10, 150: 10, 151: 4, 152: 11, 153: 7, 154: 8, 155: 7, 156: 11, 157: 9, 158: 11, 159: 4, 160: 9, 161: 8, 162: 4, 163: 8, 164: 7, 165: 6, 166: 5, 167: 6, 168: 8, 169: 11, 170: 9, 171: 4, 172: 6, 173: 6, 174: 7, 175: 6, 176: 6, 177: 6, 178: 8, 179: 4, 180: 14, 181: 4, 182: 8, 183: 6, 184: 9, 185: 7, 186: 7, 187: 7, 188: 10, 189: 13, 190: 6, 191: 5, 192: 8, 193: 5, 194: 7, 195: 5, 196: 4, 197: 7, 198: 5, 199: 7, 200: 4, 201: 5, 202: 9, 203: 6, 204: 7, 205: 8, 206: 4, 207: 8, 208: 5, 209: 10, 210: 4, 211: 5, 212: 5, 213: 6, 214: 5, 215: 6, 216: 6, 217: 6, 218: 7, 219: 6, 220: 5, 221: 4, 222: 6, 223: 9, 224: 5, 225: 7, 226: 4, 227: 10, 228: 5, 229: 4, 230: 7, 231: 4, 232: 8, 233: 6, 234: 5, 235: 4, 236: 5, 237: 5, 238: 10, 239: 7, 240: 9, 241: 6, 242: 7, 243: 8, 244: 6, 245: 5, 246: 5, 247: 6, 248: 6, 249: 5, 250: 4, 251: 7, 252: 4, 253: 5, 254: 4, 255: 5, 256: 7, 257: 8, 258: 7, 259: 11, 260: 4, 261: 4, 262: 4, 263: 4, 264: 4, 265: 5, 266: 9, 267: 4, 268: 4, 269: 5, 270: 7, 271: 5, 272: 7, 273: 5, 274: 5, 275: 4, 276: 7, 277: 4, 278: 7, 279: 6, 280: 5, 281: 4, 282: 5, 283: 4, 284: 5, 285: 6, 286: 4, 287: 6, 288: 5, 289: 4, 290: 5, 291: 4, 292: 4, 293: 4, 294: 4, 295: 5, 296: 7, 297: 5, 298: 5, 299: 4, 300: 5, 301: 4, 302: 6, 303: 4, 304: 6, 305: 5, 306: 6, 307: 5, 308: 5, 309: 5, 310: 6, 311: 5, 312: 5, 313: 4, 314: 4, 315: 6, 316: 4, 317: 6, 318: 4, 319: 4, 320: 7, 321: 6, 322: 4, 323: 5, 324: 4, 325: 4, 326: 5, 327: 5, 328: 6, 329: 4, 330: 5, 331: 5, 332: 6, 333: 5, 334: 4, 335: 5, 336: 5, 337: 4, 338: 4, 339: 5, 340: 6, 341: 5, 342: 5, 343: 4, 344: 4, 345: 4, 346: 5, 347: 4, 348: 5, 349: 4, 350: 4, 351: 5, 352: 6, 353: 6, 354: 4, 355: 5, 356: 4, 357: 5, 358: 4, 359: 5, 360: 4, 361: 5, 362: 4, 363: 4, 364: 4, 365: 5, 366: 7, 367: 5, 368: 4, 369: 4, 370: 4, 371: 4, 372: 5, 373: 4, 374: 4, 375: 4, 376: 5, 377: 5, 378: 4, 379: 4, 380: 4, 381: 4, 382: 5, 383: 4, 384: 5, 385: 5, 386: 4, 387: 4, 388: 4, 389: 4, 390: 5, 391: 5, 392: 6, 393: 5, 394: 5, 395: 5, 396: 4, 397: 4, 398: 4, 399: 5, 400: 5, 401: 4, 402: 5, 403: 4, 404: 4, 405: 5, 406: 7, 407: 5, 408: 4, 409: 4, 410: 4, 411: 5, 412: 4, 413: 4, 414: 5, 415: 4, 416: 5, 417: 4, 418: 4, 419: 5, 420: 5, 421: 4, 422: 5, 423: 4, 424: 4, 425: 5, 426: 4, 427: 4, 428: 4, 429: 4, 430: 4, 431: 4, 432: 4, 433: 4, 434: 4, 435: 5, 436: 4, 437: 4, 438: 4, 439: 4, 440: 4, 441: 4, 442: 4, 443: 5, 444: 4, 445: 4, 446: 4, 447: 5, 448: 4, 449: 4, 450: 4, 451: 4, 452: 4, 453: 4, 454: 5, 455: 4, 456: 4, 457: 5, 458: 4, 459: 4, 460: 5, 461: 4, 462: 4, 463: 4, 464: 4, 465: 4, 466: 4, 467: 4, 468: 4, 469: 4, 470: 4, 471: 4, 472: 4, 473: 4, 474: 4, 475: 4, 476: 4, 477: 4, 478: 4, 479: 4, 480: 4, 481: 4, 482: 4, 483: 4, 484: 4, 485: 4, 486: 4, 487: 4, 488: 4, 489: 4, 490: 4, 491: 4, 492: 4, 493: 4, 494: 4, 495: 4, 496: 4, 497: 4, 498: 4, 499: 4})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_g.degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[48,\n",
       " 42,\n",
       " 2,\n",
       " 41,\n",
       " 70,\n",
       " 44,\n",
       " 74,\n",
       " 40,\n",
       " 37,\n",
       " 30,\n",
       " 23,\n",
       " 20,\n",
       " 22,\n",
       " 25,\n",
       " 13,\n",
       " 21,\n",
       " 46,\n",
       " 44,\n",
       " 18,\n",
       " 25,\n",
       " 19,\n",
       " 25,\n",
       " 45,\n",
       " 14,\n",
       " 30,\n",
       " 22,\n",
       " 19,\n",
       " 19,\n",
       " 24,\n",
       " 20,\n",
       " 19,\n",
       " 19,\n",
       " 9,\n",
       " 10,\n",
       " 10,\n",
       " 6,\n",
       " 7,\n",
       " 22,\n",
       " 10,\n",
       " 8,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 19,\n",
       " 7,\n",
       " 11,\n",
       " 17,\n",
       " 7,\n",
       " 9,\n",
       " 12,\n",
       " 14,\n",
       " 12,\n",
       " 11,\n",
       " 14,\n",
       " 6,\n",
       " 12,\n",
       " 9,\n",
       " 9,\n",
       " 13,\n",
       " 6,\n",
       " 13,\n",
       " 7,\n",
       " 20,\n",
       " 7,\n",
       " 17,\n",
       " 20,\n",
       " 11,\n",
       " 8,\n",
       " 10,\n",
       " 11,\n",
       " 15,\n",
       " 15,\n",
       " 13,\n",
       " 21,\n",
       " 18,\n",
       " 9,\n",
       " 11,\n",
       " 9,\n",
       " 6,\n",
       " 9,\n",
       " 10,\n",
       " 16,\n",
       " 22,\n",
       " 6,\n",
       " 16,\n",
       " 6,\n",
       " 8,\n",
       " 7,\n",
       " 19,\n",
       " 17,\n",
       " 10,\n",
       " 12,\n",
       " 10,\n",
       " 6,\n",
       " 16,\n",
       " 11,\n",
       " 8,\n",
       " 8,\n",
       " 12,\n",
       " 9,\n",
       " 5,\n",
       " 6,\n",
       " 4,\n",
       " 8,\n",
       " 8,\n",
       " 14,\n",
       " 6,\n",
       " 5,\n",
       " 8,\n",
       " 4,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 5,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 6,\n",
       " 10,\n",
       " 5,\n",
       " 9,\n",
       " 5,\n",
       " 8,\n",
       " 7,\n",
       " 9,\n",
       " 5,\n",
       " 6,\n",
       " 9,\n",
       " 10,\n",
       " 8,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 7,\n",
       " 9,\n",
       " 15,\n",
       " 8,\n",
       " 6,\n",
       " 11,\n",
       " 11,\n",
       " 8,\n",
       " 6,\n",
       " 8,\n",
       " 8,\n",
       " 7,\n",
       " 13,\n",
       " 8,\n",
       " 7,\n",
       " 10,\n",
       " 10,\n",
       " 4,\n",
       " 11,\n",
       " 7,\n",
       " 8,\n",
       " 7,\n",
       " 11,\n",
       " 9,\n",
       " 11,\n",
       " 4,\n",
       " 9,\n",
       " 8,\n",
       " 4,\n",
       " 8,\n",
       " 7,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 8,\n",
       " 11,\n",
       " 9,\n",
       " 4,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 8,\n",
       " 4,\n",
       " 14,\n",
       " 4,\n",
       " 8,\n",
       " 6,\n",
       " 9,\n",
       " 7,\n",
       " 7,\n",
       " 7,\n",
       " 10,\n",
       " 13,\n",
       " 6,\n",
       " 5,\n",
       " 8,\n",
       " 5,\n",
       " 7,\n",
       " 5,\n",
       " 4,\n",
       " 7,\n",
       " 5,\n",
       " 7,\n",
       " 4,\n",
       " 5,\n",
       " 9,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 4,\n",
       " 8,\n",
       " 5,\n",
       " 10,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 6,\n",
       " 7,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 9,\n",
       " 5,\n",
       " 7,\n",
       " 4,\n",
       " 10,\n",
       " 5,\n",
       " 4,\n",
       " 7,\n",
       " 4,\n",
       " 8,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 10,\n",
       " 7,\n",
       " 9,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 7,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 7,\n",
       " 8,\n",
       " 7,\n",
       " 11,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 9,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 7,\n",
       " 5,\n",
       " 7,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 7,\n",
       " 4,\n",
       " 7,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 7,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 6,\n",
       " 4,\n",
       " 4,\n",
       " 7,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 6,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 7,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 6,\n",
       " 5,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 7,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 5,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 5,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4,\n",
       " 4]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[train_g.degree(i) for i in range(train_g.number_of_nodes())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nx.betweenness_centrality(train_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[100, 104],\n",
       "       [100, 108],\n",
       "       [100, 109],\n",
       "       [100, 113],\n",
       "       [100, 114],\n",
       "       [100, 115],\n",
       "       [100, 119],\n",
       "       [100, 122],\n",
       "       [100, 130],\n",
       "       [100, 150]])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.array(list(train_g.edges())) + 100)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nx.betweenness_centrality(train_g)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DrBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "# from model1 import DrBC\n",
    "from model import DrBC\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DrBC().to(device)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of DrBC(\n",
       "  (linear0): Linear(in_features=3, out_features=128, bias=True)\n",
       "  (gcn): GCNConv()\n",
       "  (gru): GRUCell(128, 128)\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(model.parameters())[9].grad.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pm0 shape: torch.Size([128, 3])\n",
      "pm1 shape: torch.Size([128])\n",
      "pm2 shape: torch.Size([384, 128])\n",
      "pm3 shape: torch.Size([384, 128])\n",
      "pm4 shape: torch.Size([384])\n",
      "pm5 shape: torch.Size([384])\n",
      "pm6 shape: torch.Size([64, 128])\n",
      "pm7 shape: torch.Size([64])\n",
      "pm8 shape: torch.Size([1, 64])\n",
      "pm9 shape: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "pm = list(model.parameters())\n",
    "\n",
    "for i, p in enumerate(pm):\n",
    "    print(f\"pm{i} shape: {p.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(dict(nx.degree(train_g)).values())\n",
    "# list(dict(nx.degree(train_g)).values())\n",
    "# list(dict(nx.betweenness_centrality(train_g)).values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Generating new training graph]: 100%|██████████| 50/50 [00:05<00:00,  8.79it/s]\n",
      "[Reading test1 graph]: 100%|██████████| 1/1 [00:00<00:00,  7.61it/s]\n",
      "Epochs 0    : 100%|██████████| 3/3 [00:00<00:00, 46.53it/s, loss=1.11e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_y:  tensor([0.0942, 0.0540, 0.0443, 0.0223, 0.0764])\n",
      "[0.1201888  0.12030778 0.1203984 ] [0.09417453 0.0539708  0.04434366 0.02232567 0.07643765]\n",
      "Val Acc: 0.0000 % | Val KendallTau: -0.5688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 1    : 100%|██████████| 3/3 [00:00<00:00, 53.49it/s, loss=1.11e+4]\n",
      "Epochs 2    : 100%|██████████| 3/3 [00:00<00:00, 51.24it/s, loss=1.11e+4]\n",
      "Epochs 3    : 100%|██████████| 3/3 [00:00<00:00, 52.59it/s, loss=1.11e+4]\n",
      "Epochs 4    : 100%|██████████| 3/3 [00:00<00:00, 51.28it/s, loss=1.11e+4]\n",
      "Epochs 5    : 100%|██████████| 3/3 [00:00<00:00, 50.04it/s, loss=1.11e+4]\n",
      "Epochs 6    : 100%|██████████| 3/3 [00:00<00:00, 51.67it/s, loss=1.11e+4]\n",
      "Epochs 7    : 100%|██████████| 3/3 [00:00<00:00, 49.42it/s, loss=1.11e+4]\n",
      "Epochs 8    : 100%|██████████| 3/3 [00:00<00:00, 50.35it/s, loss=1.11e+4]\n",
      "Epochs 9    : 100%|██████████| 3/3 [00:00<00:00, 52.23it/s, loss=1.11e+4]\n",
      "Epochs 10   : 100%|██████████| 3/3 [00:00<00:00, 50.67it/s, loss=1.11e+4]\n",
      "Epochs 11   : 100%|██████████| 3/3 [00:00<00:00, 49.96it/s, loss=1.11e+4]\n",
      "Epochs 12   : 100%|██████████| 3/3 [00:00<00:00, 48.73it/s, loss=1.11e+4]\n",
      "Epochs 13   : 100%|██████████| 3/3 [00:00<00:00, 50.55it/s, loss=1.11e+4]\n",
      "Epochs 14   : 100%|██████████| 3/3 [00:00<00:00, 46.14it/s, loss=1.11e+4]\n",
      "Epochs 15   : 100%|██████████| 3/3 [00:00<00:00, 46.42it/s, loss=1.11e+4]\n",
      "Epochs 16   : 100%|██████████| 3/3 [00:00<00:00, 43.17it/s, loss=1.11e+4]\n",
      "Epochs 17   : 100%|██████████| 3/3 [00:00<00:00, 46.20it/s, loss=1.11e+4]\n",
      "Epochs 18   : 100%|██████████| 3/3 [00:00<00:00, 47.74it/s, loss=1.11e+4]\n",
      "Epochs 19   : 100%|██████████| 3/3 [00:00<00:00, 48.74it/s, loss=1.11e+4]\n",
      "Epochs 20   : 100%|██████████| 3/3 [00:00<00:00, 44.11it/s, loss=1.11e+4]\n",
      "Epochs 21   : 100%|██████████| 3/3 [00:00<00:00, 48.87it/s, loss=1.11e+4]\n",
      "Epochs 22   : 100%|██████████| 3/3 [00:00<00:00, 50.98it/s, loss=1.11e+4]\n",
      "Epochs 23   : 100%|██████████| 3/3 [00:00<00:00, 48.75it/s, loss=1.11e+4]\n",
      "Epochs 24   : 100%|██████████| 3/3 [00:00<00:00, 49.81it/s, loss=1.11e+4]\n",
      "Epochs 25   : 100%|██████████| 3/3 [00:00<00:00, 52.67it/s, loss=1.11e+4]\n",
      "Epochs 26   : 100%|██████████| 3/3 [00:00<00:00, 48.97it/s, loss=1.11e+4]\n",
      "Epochs 27   : 100%|██████████| 3/3 [00:00<00:00, 44.74it/s, loss=1.11e+4]\n",
      "Epochs 28   : 100%|██████████| 3/3 [00:00<00:00, 46.36it/s, loss=1.11e+4]\n",
      "Epochs 29   : 100%|██████████| 3/3 [00:00<00:00, 46.24it/s, loss=1.11e+4]\n",
      "Epochs 30   : 100%|██████████| 3/3 [00:00<00:00, 51.26it/s, loss=1.11e+4]\n",
      "Epochs 31   : 100%|██████████| 3/3 [00:00<00:00, 47.66it/s, loss=1.11e+4]\n",
      "Epochs 32   : 100%|██████████| 3/3 [00:00<00:00, 48.76it/s, loss=1.11e+4]\n",
      "Epochs 33   : 100%|██████████| 3/3 [00:00<00:00, 45.30it/s, loss=1.11e+4]\n",
      "Epochs 34   : 100%|██████████| 3/3 [00:00<00:00, 49.37it/s, loss=1.11e+4]\n",
      "Epochs 35   : 100%|██████████| 3/3 [00:00<00:00, 45.74it/s, loss=1.11e+4]\n",
      "Epochs 36   : 100%|██████████| 3/3 [00:00<00:00, 50.25it/s, loss=1.11e+4]\n",
      "Epochs 37   : 100%|██████████| 3/3 [00:00<00:00, 47.03it/s, loss=1.11e+4]\n",
      "Epochs 38   : 100%|██████████| 3/3 [00:00<00:00, 47.26it/s, loss=1.11e+4]\n",
      "Epochs 39   : 100%|██████████| 3/3 [00:00<00:00, 47.58it/s, loss=1.11e+4]\n",
      "Epochs 40   : 100%|██████████| 3/3 [00:00<00:00, 50.18it/s, loss=1.11e+4]\n",
      "Epochs 41   : 100%|██████████| 3/3 [00:00<00:00, 48.50it/s, loss=1.11e+4]\n",
      "Epochs 42   : 100%|██████████| 3/3 [00:00<00:00, 48.27it/s, loss=1.11e+4]\n",
      "Epochs 43   : 100%|██████████| 3/3 [00:00<00:00, 49.96it/s, loss=1.11e+4]\n",
      "Epochs 44   : 100%|██████████| 3/3 [00:00<00:00, 44.90it/s, loss=1.11e+4]\n",
      "Epochs 45   : 100%|██████████| 3/3 [00:00<00:00, 49.43it/s, loss=1.11e+4]\n",
      "Epochs 46   : 100%|██████████| 3/3 [00:00<00:00, 46.32it/s, loss=1.11e+4]\n",
      "Epochs 47   : 100%|██████████| 3/3 [00:00<00:00, 50.11it/s, loss=1.11e+4]\n",
      "Epochs 48   : 100%|██████████| 3/3 [00:00<00:00, 53.66it/s, loss=1.11e+4]\n",
      "Epochs 49   : 100%|██████████| 3/3 [00:00<00:00, 48.89it/s, loss=1.11e+4]\n",
      "Epochs 50   : 100%|██████████| 3/3 [00:00<00:00, 55.03it/s, loss=1.11e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_y:  tensor([0.0942, 0.0540, 0.0443, 0.0223, 0.0764])\n",
      "[0.13755865 0.13730532 0.13711198] [0.09417453 0.0539708  0.04434366 0.02232567 0.07643765]\n",
      "Val Acc: 94.0000 % | Val KendallTau: 0.7397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 51   : 100%|██████████| 3/3 [00:00<00:00, 55.10it/s, loss=1.11e+4]\n",
      "Epochs 52   : 100%|██████████| 3/3 [00:00<00:00, 55.22it/s, loss=1.11e+4]\n",
      "Epochs 53   : 100%|██████████| 3/3 [00:00<00:00, 52.12it/s, loss=1.11e+4]\n",
      "Epochs 54   : 100%|██████████| 3/3 [00:00<00:00, 50.62it/s, loss=1.11e+4]\n",
      "Epochs 55   : 100%|██████████| 3/3 [00:00<00:00, 47.53it/s, loss=1.11e+4]\n",
      "Epochs 56   : 100%|██████████| 3/3 [00:00<00:00, 49.79it/s, loss=1.11e+4]\n",
      "Epochs 57   : 100%|██████████| 3/3 [00:00<00:00, 45.81it/s, loss=1.11e+4]\n",
      "Epochs 58   : 100%|██████████| 3/3 [00:00<00:00, 55.12it/s, loss=1.11e+4]\n",
      "Epochs 59   : 100%|██████████| 3/3 [00:00<00:00, 52.96it/s, loss=1.11e+4]\n",
      "Epochs 60   : 100%|██████████| 3/3 [00:00<00:00, 54.68it/s, loss=1.11e+4]\n",
      "Epochs 61   : 100%|██████████| 3/3 [00:00<00:00, 52.36it/s, loss=1.11e+4]\n",
      "Epochs 62   : 100%|██████████| 3/3 [00:00<00:00, 51.02it/s, loss=1.11e+4]\n",
      "Epochs 63   : 100%|██████████| 3/3 [00:00<00:00, 50.88it/s, loss=1.11e+4]\n",
      "Epochs 64   : 100%|██████████| 3/3 [00:00<00:00, 51.57it/s, loss=1.11e+4]\n",
      "Epochs 65   : 100%|██████████| 3/3 [00:00<00:00, 51.74it/s, loss=1.11e+4]\n",
      "Epochs 66   : 100%|██████████| 3/3 [00:00<00:00, 48.78it/s, loss=1.11e+4]\n",
      "Epochs 67   : 100%|██████████| 3/3 [00:00<00:00, 47.93it/s, loss=1.11e+4]\n",
      "Epochs 68   : 100%|██████████| 3/3 [00:00<00:00, 50.83it/s, loss=1.11e+4]\n",
      "Epochs 69   : 100%|██████████| 3/3 [00:00<00:00, 51.32it/s, loss=1.11e+4]\n",
      "Epochs 70   : 100%|██████████| 3/3 [00:00<00:00, 49.79it/s, loss=1.11e+4]\n",
      "Epochs 71   : 100%|██████████| 3/3 [00:00<00:00, 48.06it/s, loss=1.11e+4]\n",
      "Epochs 72   : 100%|██████████| 3/3 [00:00<00:00, 46.34it/s, loss=1.11e+4]\n",
      "Epochs 73   : 100%|██████████| 3/3 [00:00<00:00, 51.84it/s, loss=1.11e+4]\n",
      "Epochs 74   : 100%|██████████| 3/3 [00:00<00:00, 48.43it/s, loss=1.11e+4]\n",
      "Epochs 75   : 100%|██████████| 3/3 [00:00<00:00, 46.55it/s, loss=1.11e+4]\n",
      "Epochs 76   : 100%|██████████| 3/3 [00:00<00:00, 45.37it/s, loss=1.11e+4]\n",
      "Epochs 77   : 100%|██████████| 3/3 [00:00<00:00, 49.18it/s, loss=1.11e+4]\n",
      "Epochs 78   : 100%|██████████| 3/3 [00:00<00:00, 48.84it/s, loss=1.11e+4]\n",
      "Epochs 79   : 100%|██████████| 3/3 [00:00<00:00, 47.90it/s, loss=1.11e+4]\n",
      "Epochs 80   : 100%|██████████| 3/3 [00:00<00:00, 46.50it/s, loss=1.11e+4]\n",
      "Epochs 81   : 100%|██████████| 3/3 [00:00<00:00, 46.33it/s, loss=1.11e+4]\n",
      "Epochs 82   : 100%|██████████| 3/3 [00:00<00:00, 44.56it/s, loss=1.11e+4]\n",
      "Epochs 83   : 100%|██████████| 3/3 [00:00<00:00, 44.84it/s, loss=1.11e+4]\n",
      "Epochs 84   : 100%|██████████| 3/3 [00:00<00:00, 42.35it/s, loss=1.11e+4]\n",
      "Epochs 85   : 100%|██████████| 3/3 [00:00<00:00, 52.84it/s, loss=1.11e+4]\n",
      "Epochs 86   : 100%|██████████| 3/3 [00:00<00:00, 47.42it/s, loss=1.11e+4]\n",
      "Epochs 87   : 100%|██████████| 3/3 [00:00<00:00, 48.15it/s, loss=1.11e+4]\n",
      "Epochs 88   : 100%|██████████| 3/3 [00:00<00:00, 48.45it/s, loss=1.11e+4]\n",
      "Epochs 89   : 100%|██████████| 3/3 [00:00<00:00, 45.74it/s, loss=1.11e+4]\n",
      "Epochs 90   : 100%|██████████| 3/3 [00:00<00:00, 50.15it/s, loss=1.11e+4]\n",
      "Epochs 91   : 100%|██████████| 3/3 [00:00<00:00, 48.03it/s, loss=1.11e+4]\n",
      "Epochs 92   : 100%|██████████| 3/3 [00:00<00:00, 45.22it/s, loss=1.11e+4]\n",
      "Epochs 93   : 100%|██████████| 3/3 [00:00<00:00, 49.55it/s, loss=1.11e+4]\n",
      "Epochs 94   : 100%|██████████| 3/3 [00:00<00:00, 46.56it/s, loss=1.11e+4]\n",
      "Epochs 95   : 100%|██████████| 3/3 [00:00<00:00, 50.35it/s, loss=1.11e+4]\n",
      "Epochs 96   : 100%|██████████| 3/3 [00:00<00:00, 46.99it/s, loss=1.11e+4]\n",
      "Epochs 97   : 100%|██████████| 3/3 [00:00<00:00, 46.01it/s, loss=1.11e+4]\n",
      "Epochs 98   : 100%|██████████| 3/3 [00:00<00:00, 42.65it/s, loss=1.11e+4]\n",
      "Epochs 99   : 100%|██████████| 3/3 [00:00<00:00, 50.26it/s, loss=1.11e+4]\n",
      "Epochs 100  : 100%|██████████| 3/3 [00:00<00:00, 51.39it/s, loss=1.11e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_y:  tensor([0.0942, 0.0540, 0.0443, 0.0223, 0.0764])\n",
      "[0.15483494 0.15361322 0.15267663] [0.09417453 0.0539708  0.04434366 0.02232567 0.07643765]\n",
      "Val Acc: 98.0000 % | Val KendallTau: 0.4487\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 101  : 100%|██████████| 3/3 [00:00<00:00, 46.39it/s, loss=1.11e+4]\n",
      "Epochs 102  : 100%|██████████| 3/3 [00:00<00:00, 42.24it/s, loss=1.11e+4]\n",
      "Epochs 103  : 100%|██████████| 3/3 [00:00<00:00, 43.76it/s, loss=1.11e+4]\n",
      "Epochs 104  : 100%|██████████| 3/3 [00:00<00:00, 47.05it/s, loss=1.11e+4]\n",
      "Epochs 105  : 100%|██████████| 3/3 [00:00<00:00, 46.03it/s, loss=1.11e+4]\n",
      "Epochs 106  : 100%|██████████| 3/3 [00:00<00:00, 50.23it/s, loss=1.11e+4]\n",
      "Epochs 107  : 100%|██████████| 3/3 [00:00<00:00, 48.66it/s, loss=1.11e+4]\n",
      "Epochs 108  : 100%|██████████| 3/3 [00:00<00:00, 50.96it/s, loss=1.11e+4]\n",
      "Epochs 109  : 100%|██████████| 3/3 [00:00<00:00, 48.22it/s, loss=1.11e+4]\n",
      "Epochs 110  : 100%|██████████| 3/3 [00:00<00:00, 51.04it/s, loss=1.11e+4]\n",
      "Epochs 111  : 100%|██████████| 3/3 [00:00<00:00, 45.99it/s, loss=1.11e+4]\n",
      "Epochs 112  : 100%|██████████| 3/3 [00:00<00:00, 50.04it/s, loss=1.11e+4]\n",
      "Epochs 113  : 100%|██████████| 3/3 [00:00<00:00, 45.01it/s, loss=1.11e+4]\n",
      "Epochs 114  : 100%|██████████| 3/3 [00:00<00:00, 46.02it/s, loss=1.11e+4]\n",
      "Epochs 115  : 100%|██████████| 3/3 [00:00<00:00, 42.48it/s, loss=1.11e+4]\n",
      "Epochs 116  : 100%|██████████| 3/3 [00:00<00:00, 49.79it/s, loss=1.11e+4]\n",
      "Epochs 117  : 100%|██████████| 3/3 [00:00<00:00, 46.40it/s, loss=1.11e+4]\n",
      "Epochs 118  : 100%|██████████| 3/3 [00:00<00:00, 48.57it/s, loss=1.11e+4]\n",
      "Epochs 119  : 100%|██████████| 3/3 [00:00<00:00, 47.55it/s, loss=1.11e+4]\n",
      "Epochs 120  : 100%|██████████| 3/3 [00:00<00:00, 47.90it/s, loss=1.11e+4]\n",
      "Epochs 121  : 100%|██████████| 3/3 [00:00<00:00, 50.09it/s, loss=1.11e+4]\n",
      "Epochs 122  : 100%|██████████| 3/3 [00:00<00:00, 47.27it/s, loss=1.11e+4]\n",
      "Epochs 123  : 100%|██████████| 3/3 [00:00<00:00, 46.85it/s, loss=1.11e+4]\n",
      "Epochs 124  : 100%|██████████| 3/3 [00:00<00:00, 52.06it/s, loss=1.11e+4]\n",
      "Epochs 125  : 100%|██████████| 3/3 [00:00<00:00, 48.05it/s, loss=1.11e+4]\n",
      "Epochs 126  : 100%|██████████| 3/3 [00:00<00:00, 48.60it/s, loss=1.11e+4]\n",
      "Epochs 127  : 100%|██████████| 3/3 [00:00<00:00, 47.49it/s, loss=1.11e+4]\n",
      "Epochs 128  : 100%|██████████| 3/3 [00:00<00:00, 44.18it/s, loss=1.11e+4]\n",
      "Epochs 129  : 100%|██████████| 3/3 [00:00<00:00, 47.66it/s, loss=1.11e+4]\n",
      "Epochs 130  : 100%|██████████| 3/3 [00:00<00:00, 47.11it/s, loss=1.11e+4]\n",
      "Epochs 131  : 100%|██████████| 3/3 [00:00<00:00, 49.72it/s, loss=1.11e+4]\n",
      "Epochs 132  : 100%|██████████| 3/3 [00:00<00:00, 43.35it/s, loss=1.11e+4]\n",
      "Epochs 133  : 100%|██████████| 3/3 [00:00<00:00, 46.04it/s, loss=1.11e+4]\n",
      "Epochs 134  : 100%|██████████| 3/3 [00:00<00:00, 47.73it/s, loss=1.11e+4]\n",
      "Epochs 135  : 100%|██████████| 3/3 [00:00<00:00, 46.41it/s, loss=1.11e+4]\n",
      "Epochs 136  : 100%|██████████| 3/3 [00:00<00:00, 46.35it/s, loss=1.11e+4]\n",
      "Epochs 137  : 100%|██████████| 3/3 [00:00<00:00, 44.42it/s, loss=1.11e+4]\n",
      "Epochs 138  : 100%|██████████| 3/3 [00:00<00:00, 48.73it/s, loss=1.11e+4]\n",
      "Epochs 139  : 100%|██████████| 3/3 [00:00<00:00, 44.13it/s, loss=1.11e+4]\n",
      "Epochs 140  : 100%|██████████| 3/3 [00:00<00:00, 47.36it/s, loss=1.11e+4]\n",
      "Epochs 141  : 100%|██████████| 3/3 [00:00<00:00, 46.50it/s, loss=1.11e+4]\n",
      "Epochs 142  : 100%|██████████| 3/3 [00:00<00:00, 49.46it/s, loss=1.11e+4]\n",
      "Epochs 143  : 100%|██████████| 3/3 [00:00<00:00, 48.49it/s, loss=1.11e+4]\n",
      "Epochs 144  : 100%|██████████| 3/3 [00:00<00:00, 41.90it/s, loss=1.11e+4]\n",
      "Epochs 145  : 100%|██████████| 3/3 [00:00<00:00, 47.43it/s, loss=1.11e+4]\n",
      "Epochs 146  : 100%|██████████| 3/3 [00:00<00:00, 42.11it/s, loss=1.11e+4]\n",
      "Epochs 147  : 100%|██████████| 3/3 [00:00<00:00, 44.59it/s, loss=1.11e+4]\n",
      "Epochs 148  : 100%|██████████| 3/3 [00:00<00:00, 44.97it/s, loss=1.11e+4]\n",
      "Epochs 149  : 100%|██████████| 3/3 [00:00<00:00, 45.57it/s, loss=1.11e+4]\n",
      "Epochs 150  : 100%|██████████| 3/3 [00:00<00:00, 44.79it/s, loss=1.11e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_y:  tensor([0.0942, 0.0540, 0.0443, 0.0223, 0.0764])\n",
      "[0.22101112 0.21601504 0.21199271] [0.09417453 0.0539708  0.04434366 0.02232567 0.07643765]\n",
      "Val Acc: 98.0000 % | Val KendallTau: 0.6856\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 151  : 100%|██████████| 3/3 [00:00<00:00, 46.91it/s, loss=1.11e+4]\n",
      "Epochs 152  : 100%|██████████| 3/3 [00:00<00:00, 47.80it/s, loss=1.11e+4]\n",
      "Epochs 153  : 100%|██████████| 3/3 [00:00<00:00, 45.84it/s, loss=1.11e+4]\n",
      "Epochs 154  : 100%|██████████| 3/3 [00:00<00:00, 48.82it/s, loss=1.11e+4]\n",
      "Epochs 155  : 100%|██████████| 3/3 [00:00<00:00, 41.96it/s, loss=1.11e+4]\n",
      "Epochs 156  : 100%|██████████| 3/3 [00:00<00:00, 47.69it/s, loss=1.11e+4]\n",
      "Epochs 157  : 100%|██████████| 3/3 [00:00<00:00, 43.30it/s, loss=1.11e+4]\n",
      "Epochs 158  : 100%|██████████| 3/3 [00:00<00:00, 49.31it/s, loss=1.11e+4]\n",
      "Epochs 159  : 100%|██████████| 3/3 [00:00<00:00, 48.47it/s, loss=1.11e+4]\n",
      "Epochs 160  : 100%|██████████| 3/3 [00:00<00:00, 40.53it/s, loss=1.11e+4]\n",
      "Epochs 161  : 100%|██████████| 3/3 [00:00<00:00, 47.25it/s, loss=1.11e+4]\n",
      "Epochs 162  : 100%|██████████| 3/3 [00:00<00:00, 45.11it/s, loss=1.11e+4]\n",
      "Epochs 163  : 100%|██████████| 3/3 [00:00<00:00, 46.34it/s, loss=1.11e+4]\n",
      "Epochs 164  : 100%|██████████| 3/3 [00:00<00:00, 47.74it/s, loss=1.11e+4]\n",
      "Epochs 165  : 100%|██████████| 3/3 [00:00<00:00, 47.03it/s, loss=1.11e+4]\n",
      "Epochs 166  : 100%|██████████| 3/3 [00:00<00:00, 41.33it/s, loss=1.11e+4]\n",
      "Epochs 167  : 100%|██████████| 3/3 [00:00<00:00, 48.51it/s, loss=1.11e+4]\n",
      "Epochs 168  : 100%|██████████| 3/3 [00:00<00:00, 52.82it/s, loss=1.11e+4]\n",
      "Epochs 169  : 100%|██████████| 3/3 [00:00<00:00, 45.69it/s, loss=1.11e+4]\n",
      "Epochs 170  : 100%|██████████| 3/3 [00:00<00:00, 45.81it/s, loss=1.11e+4]\n",
      "Epochs 171  : 100%|██████████| 3/3 [00:00<00:00, 46.29it/s, loss=1.11e+4]\n",
      "Epochs 172  : 100%|██████████| 3/3 [00:00<00:00, 46.82it/s, loss=1.11e+4]\n",
      "Epochs 173  : 100%|██████████| 3/3 [00:00<00:00, 45.01it/s, loss=1.11e+4]\n",
      "Epochs 174  : 100%|██████████| 3/3 [00:00<00:00, 41.36it/s, loss=1.11e+4]\n",
      "Epochs 175  : 100%|██████████| 3/3 [00:00<00:00, 46.48it/s, loss=1.11e+4]\n",
      "Epochs 176  : 100%|██████████| 3/3 [00:00<00:00, 42.36it/s, loss=1.11e+4]\n",
      "Epochs 177  : 100%|██████████| 3/3 [00:00<00:00, 50.16it/s, loss=1.11e+4]\n",
      "Epochs 178  : 100%|██████████| 3/3 [00:00<00:00, 43.93it/s, loss=1.11e+4]\n",
      "Epochs 179  : 100%|██████████| 3/3 [00:00<00:00, 38.25it/s, loss=1.11e+4]\n",
      "Epochs 180  : 100%|██████████| 3/3 [00:00<00:00, 48.67it/s, loss=1.11e+4]\n",
      "Epochs 181  : 100%|██████████| 3/3 [00:00<00:00, 45.69it/s, loss=1.11e+4]\n",
      "Epochs 182  : 100%|██████████| 3/3 [00:00<00:00, 45.69it/s, loss=1.11e+4]\n",
      "Epochs 183  : 100%|██████████| 3/3 [00:00<00:00, 45.36it/s, loss=1.11e+4]\n",
      "Epochs 184  : 100%|██████████| 3/3 [00:00<00:00, 46.19it/s, loss=1.11e+4]\n",
      "Epochs 185  : 100%|██████████| 3/3 [00:00<00:00, 48.93it/s, loss=1.11e+4]\n",
      "Epochs 186  : 100%|██████████| 3/3 [00:00<00:00, 49.21it/s, loss=1.11e+4]\n",
      "Epochs 187  : 100%|██████████| 3/3 [00:00<00:00, 44.29it/s, loss=1.11e+4]\n",
      "Epochs 188  : 100%|██████████| 3/3 [00:00<00:00, 50.35it/s, loss=1.11e+4]\n",
      "Epochs 189  : 100%|██████████| 3/3 [00:00<00:00, 46.74it/s, loss=1.11e+4]\n",
      "Epochs 190  : 100%|██████████| 3/3 [00:00<00:00, 50.85it/s, loss=1.11e+4]\n",
      "Epochs 191  : 100%|██████████| 3/3 [00:00<00:00, 45.02it/s, loss=1.11e+4]\n",
      "Epochs 192  : 100%|██████████| 3/3 [00:00<00:00, 49.60it/s, loss=1.11e+4]\n",
      "Epochs 193  : 100%|██████████| 3/3 [00:00<00:00, 41.20it/s, loss=1.11e+4]\n",
      "Epochs 194  : 100%|██████████| 3/3 [00:00<00:00, 45.63it/s, loss=1.11e+4]\n",
      "Epochs 195  : 100%|██████████| 3/3 [00:00<00:00, 44.45it/s, loss=1.11e+4]\n",
      "Epochs 196  : 100%|██████████| 3/3 [00:00<00:00, 47.53it/s, loss=1.11e+4]\n",
      "Epochs 197  : 100%|██████████| 3/3 [00:00<00:00, 47.39it/s, loss=1.11e+4]\n",
      "Epochs 198  : 100%|██████████| 3/3 [00:00<00:00, 47.69it/s, loss=1.11e+4]\n",
      "Epochs 199  : 100%|██████████| 3/3 [00:00<00:00, 46.13it/s, loss=1.11e+4]\n",
      "Epochs 200  : 100%|██████████| 3/3 [00:00<00:00, 47.93it/s, loss=1.11e+4]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_y:  tensor([0.0942, 0.0540, 0.0443, 0.0223, 0.0764])\n",
      "[0.23258547 0.22666341 0.22184072] [0.09417453 0.0539708  0.04434366 0.02232567 0.07643765]\n",
      "Val Acc: 98.0000 % | Val KendallTau: 0.7020\n"
     ]
    }
   ],
   "source": [
    "def validate(model, v_data):\n",
    "    model.eval()\n",
    "    total_acc = 0.\n",
    "    total_kendall = 0.\n",
    "    for val_X, val_y, val_edge_index in v_data:\n",
    "        val_X, val_edge_index = val_X.to(device), val_edge_index.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            val_y_pred = model(val_X, val_edge_index)\n",
    "\n",
    "        # print('val_X: ', val_X[:5])\n",
    "        # print('val_edge_list: ', val_edge_index[:, -5:])\n",
    "        # print(val_edge_index.shape)\n",
    "        # print('pred_y: ', val_y_pred[:5])\n",
    "        print('val_y: ', val_y[:5])\n",
    "        # return\n",
    "        val_y_pred = val_y_pred.cpu().detach().numpy()\n",
    "        val_y = val_y.detach().numpy()\n",
    "\n",
    "        pred_index = val_y_pred.argsort()[::-1]\n",
    "        true_index = val_y.argsort()[::-1]\n",
    "        \n",
    "        acc = top_n_acc(pred_index, true_index)\n",
    "        kendall_t, _ = stats.kendalltau(val_y_pred, val_y)\n",
    "        print(val_y_pred[:3], val_y[:5])\n",
    "\n",
    "        total_acc += acc\n",
    "        total_kendall += kendall_t\n",
    "\n",
    "    total_acc /= len(v_data)\n",
    "    total_kendall /= len(v_data)\n",
    "    return total_acc, total_kendall\n",
    "    \n",
    "\n",
    "def train(model, optim, loss_fn, epochs:int):\n",
    "    g_list, dg_list, bc_list  = prepare_synthetic(SYNTHETIC_NUM, (NUM_MIN, NUM_MAX))\n",
    "    v_data = prepare_test1(TEST1_NUM)\n",
    "    \n",
    "    ls_metric = []\n",
    "    batch_cnt = len(g_list) // BATCH_SIZE\n",
    "    for e in range(epochs + 1):\n",
    "        model.train()\n",
    "        g_list, dg_list, bc_list = shuffle_graph(g_list, dg_list, bc_list)\n",
    "        batch_bar = tqdm(range(batch_cnt))\n",
    "        batch_bar.set_description(f'Epochs {e:<5}')\n",
    "        train_loss = 0\n",
    "        pair_cnt = 0\n",
    "        for i in batch_bar:\n",
    "            # batch\n",
    "            s_index, e_index = i*BATCH_SIZE, (i+1)*BATCH_SIZE\n",
    "            train_g, train_dg, train_bc = g_list[s_index: e_index], dg_list[s_index: e_index], bc_list[s_index: e_index]\n",
    "            X, y, edge_index = preprocessing_data(train_g, train_dg, train_bc)\n",
    "            X, y, edge_index = X.to(device), y.to(device), edge_index.to(device)\n",
    "            out = model(X, edge_index)\n",
    "\n",
    "            # pairwise-loss\n",
    "            s_ids, t_ids = get_pairwise_ids(train_g)\n",
    "            out_diff = out[s_ids] - out[t_ids]\n",
    "            y_diff = y[s_ids] - y[t_ids]\n",
    "            loss = loss_fn(out_diff, torch.sigmoid(y_diff))\n",
    "\n",
    "            # optim\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            pair_cnt += s_ids.shape[0]\n",
    "            train_loss += (loss.item() * s_ids.shape[0])\n",
    "            if i == (batch_cnt - 1):\n",
    "                # last batch\n",
    "                train_loss /= pair_cnt\n",
    "                batch_bar.set_postfix(loss=round(train_loss, 6)) \n",
    "\n",
    "        if e % 50 == 0:\n",
    "            # print('out: ', out[:10])\n",
    "            val_acc, val_kendall = validate(model, v_data)\n",
    "            ls_metric.append([e, val_acc, val_kendall])\n",
    "            print(f\"Val Acc: {val_acc * 100:.4f} % | Val KendallTau: {val_kendall:.4f}\")\n",
    "        \n",
    "\n",
    "_ = train(model, optim, loss_fn, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read file\n",
    "import urllib.request  \n",
    "\n",
    "class readFile():\n",
    "  def __init__(self,file):\n",
    "    if file == 'y':\n",
    "      url1 = 'https://raw.githubusercontent.com/emschenn/mlg_hw1/master/hw1_data/youtube/com-youtube.txt' \n",
    "      url2 = 'https://raw.githubusercontent.com/emschenn/mlg_hw1/master/hw1_data/youtube/com-youtube_score.txt' \n",
    "    else:\n",
    "      url1 = 'https://raw.githubusercontent.com/emschenn/mlg_hw1/master/hw1_data/Synthetic/5000/' + file + '.txt'\n",
    "      url2 = 'https://raw.githubusercontent.com/emschenn/mlg_hw1/master/hw1_data/Synthetic/5000/' + file + '_score.txt'\n",
    "    self.bc_value,s_list,t_list,self.deg_list,n = [],[],[],[],0\n",
    "    for line in urllib.request.urlopen(url2):\n",
    "      _,v = line.decode('utf-8').split()\n",
    "      self.bc_value.append([n,float(v)])\n",
    "      n += 1\n",
    "    for x in range(len(self.bc_value)):\n",
    "      self.deg_list.append([0,1,1])\n",
    "    for line in urllib.request.urlopen(url1):\n",
    "      s,t = line.decode('utf-8').split()\n",
    "      s,t = int(s),int(t)\n",
    "      s_list.append(s)\n",
    "      t_list.append(t)\n",
    "      self.deg_list[s][0]+=1\n",
    "      self.deg_list[t][0]+=1\n",
    "    # self.edge_index=[s_list+t_list,t_list+s_list]\n",
    "    self.edge_index=[s_list,t_list]\n",
    "\n",
    "  def get_deg_list(self):\n",
    "    # print(self.deg_list)\n",
    "    return torch.Tensor(self.deg_list).cuda()\n",
    "\n",
    "  def get_edge_index(self):\n",
    "    # print(self.edge_index)\n",
    "    return torch.tensor(self.edge_index,dtype=torch.long).cuda()\n",
    "\n",
    "  def get_bc_value(self):\n",
    "    # print(self.bc_value)\n",
    "    return self.bc_value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5000, 3]) torch.Size([2, 19982])\n",
      "val_X:  tensor([[239.,   1.,   1.],\n",
      "        [178.,   1.,   1.],\n",
      "        [149.,   1.,   1.],\n",
      "        [ 90.,   1.,   1.],\n",
      "        [196.,   1.,   1.]], device='cuda:0')\n",
      "val_edge_list:  tensor([[4823, 4828, 4844, 4870, 4937],\n",
      "        [4987, 4968, 4849, 4928, 4953]], device='cuda:0')\n",
      "pred_y:  tensor([0.2326, 0.2267, 0.2218, 0.2004, 0.2396], device='cuda:0')\n",
      "val_y:  [[0, 0.09417453090592563], [1, 0.05397079661985897], [2, 0.04434365787783783], [3, 0.022325672571532364], [4, 0.0764376504965615]]\n",
      "0.98\n",
      "val_y:  [[0, 0.09417453090592563], [5, 0.092789552991686], [4, 0.0764376504965615], [1, 0.05397079661985897], [6, 0.05002370607942536]]\n",
      "0.884\n",
      "val_y:  [[0, 0.09417453090592563], [5, 0.092789552991686], [4, 0.0764376504965615], [1, 0.05397079661985897], [6, 0.05002370607942536]]\n",
      "0.852\n",
      "[0.23258547484874725, 0.22666341066360474, 0.2218407243490219] [0.09417453090592563, 0.092789552991686, 0.0764376504965615]\n",
      "[[0, 0.09417453090592563], [5, 0.092789552991686], [4, 0.0764376504965615]]\n",
      "0.5403945631494651\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "f = readFile('0')\n",
    "model = model\n",
    "t = f.get_deg_list()\n",
    "t1 = f.get_edge_index()\n",
    "print(t.shape, t1.shape)\n",
    "with torch.no_grad():\n",
    "  outs = model(t,t1)\n",
    "  print('val_X: ', t[:5])\n",
    "  print('val_edge_list: ', t1[:, -5:])\n",
    "  print('pred_y: ', outs[:5])\n",
    "\n",
    "# Top-N % accuracy\n",
    "def takeSecond(elem):\n",
    "    return elem[1]\n",
    "\n",
    "def topN_accuracy(file,outs,n):\n",
    "  predict_value,bc_value = [],[]\n",
    "  for i,j in enumerate(outs.tolist()):\n",
    "    predict_value.append([i,j])\n",
    "  bc_value = f.get_bc_value()\n",
    "  print('val_y: ', bc_value[:5])\n",
    "  bc_value.sort(key = takeSecond,reverse = True)\n",
    "  predict_value.sort(key = takeSecond,reverse = True)\n",
    "  p,t = [],[]\n",
    "  for x in range(int(len(predict_value)*n/100)):\n",
    "    p.append(predict_value[x][0])\n",
    "    t.append(bc_value[x][0])\n",
    "  # print(t)\n",
    "  # print(p)\n",
    "  return(len(set(t)&set(p)) / len(p))\n",
    "\n",
    "print(topN_accuracy(f,outs,n=1))\n",
    "print(topN_accuracy(f,outs,n=5))\n",
    "print(topN_accuracy(f,outs,n=10))\n",
    "\n",
    "# Kendall tau\n",
    "import scipy.stats as stats\n",
    "def kendall_tau(file,outs):\n",
    "  predict_value,bc_value = [],[]\n",
    "  for i,j in enumerate(outs.tolist()):\n",
    "    predict_value.append(j)\n",
    "  for i in f.get_bc_value():\n",
    "    bc_value.append(i[1])\n",
    "  # print(predict_value)\n",
    "  # print(bc_value)\n",
    "  tau, _ = stats.kendalltau(predict_value, bc_value)\n",
    "  print(predict_value[:3], bc_value[:3])\n",
    "  print(f.get_bc_value()[:3])\n",
    "  return(tau)\n",
    "\n",
    "# def kendall_tau(file,outs):\n",
    "#   predict_value,bc_value = [],[]\n",
    "#   for i,j in enumerate(outs.tolist()):\n",
    "#     predict_value.append(*j)\n",
    "#   for i in file.get_bc_value():\n",
    "#     bc_value.append(i[1])\n",
    "#   # print(predict_value)\n",
    "#   # print(bc_value)\n",
    "#   tau, _ = stats.kendalltau(predict_value, bc_value)\n",
    "#   return(tau)\n",
    "\n",
    "print(kendall_tau(f,outs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1758474167.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn [109], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    [0.1677003  0.16370043 0.15999609] [0.09417453 0.0539708  0.04434366]\u001b[0m\n\u001b[0m                ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "val_y:  tensor([0.0942, 0.0540, 0.0443, 0.0223, 0.0764])\n",
    "[0.23258547 0.22666341 0.22184072] [0.09417453 0.0539708  0.04434366 0.02232567 0.07643765]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# g = _[2]\n",
    "# g.degree(list(range(99, 105)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To-Do List\n",
    "* (done) loss_fn 再加上 sigmoid\n",
    "* (done) pairwise 目前跨圖了\n",
    "* (done) h 要 normalized\n",
    "* (done) aggregate 改成 MessagePassing\n",
    "* (done) synthetic graph 後，shuffle graph 的順序\n",
    "* (done) 加入 Epochs\n",
    "* Metric: top1, 5, 10\n",
    "* Metric: kendall tau distance\n",
    "* wall-clock running time\n",
    "* test step\n",
    "* (done) change to leaky relu -> back to relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyterhub",
   "language": "python",
   "name": "jupyterhub"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
