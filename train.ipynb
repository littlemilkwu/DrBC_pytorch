{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import read_test_data\n",
    "from utils import gen_graph\n",
    "from utils import prepare_synthetic\n",
    "from utils import shuffle_graph\n",
    "from utils import preprocessing_data\n",
    "from utils import get_pairwise_ids\n",
    "\n",
    "from utils import prepare_test\n",
    "from utils import top_n_acc\n",
    "from utils import validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 11\n",
    "# SYNTHETIC_NUM = 16\n",
    "SYNTHETIC_NUM = 1000\n",
    "\n",
    "\n",
    "# number of gen nodes\n",
    "# NUM_MIN = 4000\n",
    "# NUM_MAX = 4001\n",
    "NUM_MIN = 5000\n",
    "NUM_MAX = 5001\n",
    "IS_PARALLEL = True if NUM_MIN >= 1000 else False\n",
    "\n",
    "\n",
    "MAX_EPOCHS = 10000\n",
    "LEARNING_RATE = 1e-4\n",
    "EMBEDDING_SIZE = 128\n",
    "DEPTH = 5\n",
    "BATCH_SIZE = 16\n",
    "\n",
    "TEST1_NUM = 30\n",
    "\n",
    "MODEL_SAVED_PATH = \"saved_model/\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1_g, test1_bc, test1_edgeindex = read_test_data(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Synthetic Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1983\n"
     ]
    }
   ],
   "source": [
    "train_g = gen_graph(500, 501)\n",
    "print(len(train_g.edges()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [train_g.degree(i) for i in range(train_g.number_of_nodes())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nx.betweenness_centrality(train_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (np.array(list(train_g.edges())) + 100)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nx.betweenness_centrality(train_g)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DrBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from model1 import DrBC\n",
    "from model import DrBC\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DrBC().to(device)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of DrBC(\n",
       "  (linear0): Linear(in_features=3, out_features=128, bias=True)\n",
       "  (gcn): GCNConv()\n",
       "  (gru): GRUCell(128, 128)\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (1): LeakyReLU(negative_slope=0.01)\n",
       "    (2): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list(model.parameters())[9].grad.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pm0 shape: torch.Size([128, 3])\n",
      "pm1 shape: torch.Size([128])\n",
      "pm2 shape: torch.Size([384, 128])\n",
      "pm3 shape: torch.Size([384, 128])\n",
      "pm4 shape: torch.Size([384])\n",
      "pm5 shape: torch.Size([384])\n",
      "pm6 shape: torch.Size([64, 128])\n",
      "pm7 shape: torch.Size([64])\n",
      "pm8 shape: torch.Size([1, 64])\n",
      "pm9 shape: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "pm = list(model.parameters())\n",
    "\n",
    "for i, p in enumerate(pm):\n",
    "    print(f\"pm{i} shape: {p.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Generating new training graph]: 100%|██████████| 1000/1000 [7:57:26<00:00, 28.65s/it] \n",
      "[Reading test1 graph]: 100%|██████████| 30/30 [00:12<00:00,  2.50it/s]\n",
      "Epochs:   0%|          | 1/10000 [00:01<5:13:57,  1.88s/it, loss=0.867]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0] Val Acc1: 0.00 % | Acc5: 0.00 % | Acc10: 0.00 % | KendallTau: -0.6277 | spend: 0.2 secs\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   5%|▌         | 501/10000 [02:37<55:15,  2.87it/s, loss=0.502] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[500] Val Acc1: 93.27 % | Acc5: 94.49 % | Acc10: 93.57 % | KendallTau: 0.8674 | spend: 0.19 secs\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  10%|█         | 1001/10000 [05:09<52:34,  2.85it/s, loss=0.501] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000] Val Acc1: 93.73 % | Acc5: 94.69 % | Acc10: 93.75 % | KendallTau: 0.8787 | spend: 0.19 secs\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  15%|█▌        | 1501/10000 [07:37<49:23,  2.87it/s, loss=0.499]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1500] Val Acc1: 95.20 % | Acc5: 94.75 % | Acc10: 93.77 % | KendallTau: 0.8809 | spend: 0.19 secs\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  20%|██        | 2001/10000 [10:04<47:05,  2.83it/s, loss=0.499]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000] Val Acc1: 95.80 % | Acc5: 94.71 % | Acc10: 93.75 % | KendallTau: 0.8814 | spend: 0.19 secs\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  25%|██▌       | 2501/10000 [12:31<43:21,  2.88it/s, loss=0.5]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2500] Val Acc1: 96.20 % | Acc5: 94.67 % | Acc10: 93.79 % | KendallTau: 0.8817 | spend: 0.19 secs\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  30%|███       | 3001/10000 [14:59<41:33,  2.81it/s, loss=0.5]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3000] Val Acc1: 96.00 % | Acc5: 94.71 % | Acc10: 93.77 % | KendallTau: 0.8820 | spend: 0.19 secs\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  35%|███▌      | 3501/10000 [17:28<38:25,  2.82it/s, loss=0.502]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3500] Val Acc1: 96.13 % | Acc5: 94.71 % | Acc10: 93.77 % | KendallTau: 0.8822 | spend: 0.19 secs\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  40%|████      | 4001/10000 [19:55<35:01,  2.86it/s, loss=0.5]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4000] Val Acc1: 96.33 % | Acc5: 94.73 % | Acc10: 93.78 % | KendallTau: 0.8823 | spend: 0.19 secs\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  45%|████▌     | 4501/10000 [22:22<32:11,  2.85it/s, loss=0.5]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4500] Val Acc1: 96.33 % | Acc5: 94.68 % | Acc10: 93.81 % | KendallTau: 0.8825 | spend: 0.19 secs\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Generating new training graph]:  22%|██▏       | 215/1000 [2:23:30<8:43:58, 40.05s/it]  \n",
      "Epochs:  50%|█████     | 5000/10000 [2:48:25<2:48:25,  2.02s/it, loss=0.5]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/littlemilk/MLG/HW1/train.ipynb Cell 18\u001b[0m in \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnetai_940_2_littlemilk/home/littlemilk/MLG/HW1/train.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=80'>81</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m-\u001b[39m\u001b[39m'\u001b[39m\u001b[39m*\u001b[39m\u001b[39m50\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnetai_940_2_littlemilk/home/littlemilk/MLG/HW1/train.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=82'>83</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m ls_metric\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bnetai_940_2_littlemilk/home/littlemilk/MLG/HW1/train.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=84'>85</a>\u001b[0m train_metric \u001b[39m=\u001b[39m train(model, optim, loss_fn, MAX_EPOCHS)\n",
      "\u001b[1;32m/home/littlemilk/MLG/HW1/train.ipynb Cell 18\u001b[0m in \u001b[0;36m5\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnetai_940_2_littlemilk/home/littlemilk/MLG/HW1/train.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=46'>47</a>\u001b[0m \u001b[39mfor\u001b[39;00m e \u001b[39min\u001b[39;00m epoch_bar:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnetai_940_2_littlemilk/home/littlemilk/MLG/HW1/train.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=47'>48</a>\u001b[0m     \u001b[39mif\u001b[39;00m (e \u001b[39m%\u001b[39m \u001b[39m5000\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m) \u001b[39mand\u001b[39;00m (e \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnetai_940_2_littlemilk/home/littlemilk/MLG/HW1/train.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=48'>49</a>\u001b[0m         \u001b[39m# re generate synthetic graph\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bnetai_940_2_littlemilk/home/littlemilk/MLG/HW1/train.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=49'>50</a>\u001b[0m         g_list, dg_list, bc_list  \u001b[39m=\u001b[39m prepare_synthetic(SYNTHETIC_NUM, (NUM_MIN, NUM_MAX), IS_PARALLEL)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnetai_940_2_littlemilk/home/littlemilk/MLG/HW1/train.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=50'>51</a>\u001b[0m     model\u001b[39m.\u001b[39mtrain()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnetai_940_2_littlemilk/home/littlemilk/MLG/HW1/train.ipynb#X24sdnNjb2RlLXJlbW90ZQ%3D%3D?line=51'>52</a>\u001b[0m     g_list, dg_list, bc_list \u001b[39m=\u001b[39m shuffle_graph(g_list, dg_list, bc_list)\n",
      "File \u001b[0;32m~/MLG/HW1/utils.py:94\u001b[0m, in \u001b[0;36mprepare_synthetic\u001b[0;34m(synthetic_num, num_range, parallel)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[39m# bc_ = list(dict(nx.betweenness_centrality(g)).values())\u001b[39;00m\n\u001b[1;32m     93\u001b[0m \u001b[39mif\u001b[39;00m parallel \u001b[39m==\u001b[39m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m---> 94\u001b[0m     dict_bc \u001b[39m=\u001b[39m betweenness_centrality_parallel(g)\n\u001b[1;32m     95\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     96\u001b[0m     dict_bc \u001b[39m=\u001b[39m nx\u001b[39m.\u001b[39mbetweenness_centrality(g)\n",
      "File \u001b[0;32m~/MLG/HW1/utils.py:61\u001b[0m, in \u001b[0;36mbetweenness_centrality_parallel\u001b[0;34m(G, processes)\u001b[0m\n\u001b[1;32m     59\u001b[0m node_chunks \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(chunks(G\u001b[39m.\u001b[39mnodes(), G\u001b[39m.\u001b[39morder() \u001b[39m/\u001b[39m\u001b[39m/\u001b[39m node_divisor))\n\u001b[1;32m     60\u001b[0m num_chunks \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(node_chunks)\n\u001b[0;32m---> 61\u001b[0m bt_sc \u001b[39m=\u001b[39m p\u001b[39m.\u001b[39;49mstarmap(\n\u001b[1;32m     62\u001b[0m     nx\u001b[39m.\u001b[39;49mbetweenness_centrality_subset,\n\u001b[1;32m     63\u001b[0m     \u001b[39mzip\u001b[39;49m(\n\u001b[1;32m     64\u001b[0m         [G] \u001b[39m*\u001b[39;49m num_chunks,\n\u001b[1;32m     65\u001b[0m         node_chunks,\n\u001b[1;32m     66\u001b[0m         [\u001b[39mlist\u001b[39;49m(G)] \u001b[39m*\u001b[39;49m num_chunks,\n\u001b[1;32m     67\u001b[0m         [\u001b[39mTrue\u001b[39;49;00m] \u001b[39m*\u001b[39;49m num_chunks,\n\u001b[1;32m     68\u001b[0m         [\u001b[39mNone\u001b[39;49;00m] \u001b[39m*\u001b[39;49m num_chunks,\n\u001b[1;32m     69\u001b[0m     ),\n\u001b[1;32m     70\u001b[0m )\n\u001b[1;32m     72\u001b[0m \u001b[39m# Reduce the partial solutions\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[39m# print(bt_sc)\u001b[39;00m\n\u001b[1;32m     74\u001b[0m bt_c \u001b[39m=\u001b[39m bt_sc[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/jupyterhub/lib/python3.9/multiprocessing/pool.py:372\u001b[0m, in \u001b[0;36mPool.starmap\u001b[0;34m(self, func, iterable, chunksize)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstarmap\u001b[39m(\u001b[39mself\u001b[39m, func, iterable, chunksize\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    367\u001b[0m     \u001b[39m'''\u001b[39;00m\n\u001b[1;32m    368\u001b[0m \u001b[39m    Like `map()` method but the elements of the `iterable` are expected to\u001b[39;00m\n\u001b[1;32m    369\u001b[0m \u001b[39m    be iterables as well and will be unpacked as arguments. Hence\u001b[39;00m\n\u001b[1;32m    370\u001b[0m \u001b[39m    `func` and (a, b) becomes func(a, b).\u001b[39;00m\n\u001b[1;32m    371\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m--> 372\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_map_async(func, iterable, starmapstar, chunksize)\u001b[39m.\u001b[39;49mget()\n",
      "File \u001b[0;32m~/miniconda3/envs/jupyterhub/lib/python3.9/multiprocessing/pool.py:765\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 765\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    766\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mready():\n\u001b[1;32m    767\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTimeoutError\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/jupyterhub/lib/python3.9/multiprocessing/pool.py:762\u001b[0m, in \u001b[0;36mApplyResult.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwait\u001b[39m(\u001b[39mself\u001b[39m, timeout\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 762\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_event\u001b[39m.\u001b[39;49mwait(timeout)\n",
      "File \u001b[0;32m~/miniconda3/envs/jupyterhub/lib/python3.9/threading.py:581\u001b[0m, in \u001b[0;36mEvent.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    579\u001b[0m signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flag\n\u001b[1;32m    580\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m signaled:\n\u001b[0;32m--> 581\u001b[0m     signaled \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_cond\u001b[39m.\u001b[39;49mwait(timeout)\n\u001b[1;32m    582\u001b[0m \u001b[39mreturn\u001b[39;00m signaled\n",
      "File \u001b[0;32m~/miniconda3/envs/jupyterhub/lib/python3.9/threading.py:312\u001b[0m, in \u001b[0;36mCondition.wait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[39mtry\u001b[39;00m:    \u001b[39m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 312\u001b[0m         waiter\u001b[39m.\u001b[39;49macquire()\n\u001b[1;32m    313\u001b[0m         gotit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    314\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train(model, optim, loss_fn, epochs:int):\n",
    "    g_list, dg_list, bc_list  = prepare_synthetic(SYNTHETIC_NUM, (NUM_MIN, NUM_MAX), IS_PARALLEL)\n",
    "    v_data = prepare_test(TEST1_NUM)\n",
    "    ls_metric = []\n",
    "    epoch_bar = tqdm(range(epochs), desc=\"Epochs\")\n",
    "    for e in epoch_bar:\n",
    "        if (e % 5000 == 0) and (e != 0):\n",
    "            # re generate synthetic graph\n",
    "            g_list, dg_list, bc_list  = prepare_synthetic(SYNTHETIC_NUM, (NUM_MIN, NUM_MAX), IS_PARALLEL)\n",
    "        model.train()\n",
    "        g_list, dg_list, bc_list = shuffle_graph(g_list, dg_list, bc_list)\n",
    "        train_g, train_dg, train_bc = g_list[:16], dg_list[:16], bc_list[:16]\n",
    "        X, y, edge_index = preprocessing_data(train_g, train_dg, train_bc)\n",
    "        X, y, edge_index = X.to(device), y.to(device), edge_index.to(device)\n",
    "        out = model(X, edge_index)\n",
    "\n",
    "        # pairwise-loss\n",
    "        s_ids, t_ids = get_pairwise_ids(train_g)\n",
    "        out_diff = out[s_ids] - out[t_ids]\n",
    "        y_diff = y[s_ids] - y[t_ids]\n",
    "        loss = loss_fn(out_diff, torch.sigmoid(y_diff))\n",
    "\n",
    "        # optim\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "\n",
    "        epoch_bar.set_postfix(loss=loss.item())\n",
    "        if e % 500 == 0:\n",
    "            # validate\n",
    "            val_acc1, val_acc5, val_acc10, val_kendall, time_spent = validate(model, v_data)\n",
    "            ls_metric.append([e, val_acc1, val_acc5, val_acc10, val_kendall, time_spent])\n",
    "            print(f\"[{e}] Val Acc1: {val_acc1 * 100:.2f} % | Acc5: {val_acc5 * 100:.2f} % | Acc10: {val_acc10 * 100:.2f} % | KendallTau: {val_kendall:.4f} | spend: {time_spent} secs\")\n",
    "            print('-'*50)\n",
    "\n",
    "    # last time \n",
    "    val_acc1, val_acc5, val_acc10, val_kendall, time_spent = validate(model, v_data)\n",
    "    ls_metric.append([epochs, val_acc1, val_acc5, val_acc10, val_kendall, time_spent])\n",
    "    print(f\"[{epochs}] Val Acc1: {val_acc1 * 100:.2f} % | Acc5: {val_acc5 * 100:.2f} % | Acc10: {val_acc10 * 100:.2f} % | KendallTau: {val_kendall:.4f} | spend: {time_spent} secs\")\n",
    "    print('-'*50)\n",
    "\n",
    "    return ls_metric\n",
    "\n",
    "train_metric = train(model, optim, loss_fn, MAX_EPOCHS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model / train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_saved_name = f'{MODEL_SAVED_PATH}DrBC_G{SYNTHETIC_NUM}_N{NUM_MIN}_E{MAX_EPOCHS}.pth'\n",
    "torch.save(model.state_dict(), model_saved_name)\n",
    "\n",
    "# train\n",
    "df = pd.DataFrame(train_metric, columns=['epochs', 'val_acc1', 'val_acc5', 'val_acc10', 'val_kendall', 'time'])\n",
    "df.to_csv(f\"{MODEL_SAVED_PATH}train_metrics_G{SYNTHETIC_NUM}_N{NUM_MIN}_E{MAX_EPOCHS}.csv\", index=False)\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = DrBC().to(device)\n",
    "model.load_state_dict(torch.load(model_saved_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G1000_N200_E10000\n",
    "# acc: \n",
    "# 0.600546\n",
    "# 0.601509\n",
    "# 0.636026\n",
    "# kendall:  0.527324\n",
    "\n",
    "# G1000_N5000_E5000\n",
    "# acc: \n",
    "# 0.61491\n",
    "# 0.633318\n",
    "# 0.66736\n",
    "# kendall:  0.513669"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# synthetic graph num: 100\n",
    "# synthetic node num: 200\n",
    "# epoch: 200\n",
    "\n",
    "\n",
    "# with L2 norm\n",
    "# acc: \n",
    "# 0.613588\n",
    "# 0.495506\n",
    "# 0.302029\n",
    "# kendall:  -0.435382\n",
    "\n",
    "# without L2 norm + bc apply log\n",
    "# 0.615791\n",
    "# 0.618709\n",
    "# 0.643578\n",
    "# kendall:  0.288244\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To-Do List\n",
    "* (done) loss_fn 再加上 sigmoid\n",
    "* (done) pairwise 目前跨圖了\n",
    "* (done) h 要 normalized\n",
    "* (done) aggregate 改成 MessagePassing\n",
    "* (done) synthetic graph 後，shuffle graph 的順序\n",
    "* (done) 加入 Epochs\"\n",
    "* (done) change to leaky relu\n",
    "* (done) Metric: top1, 5, 10\n",
    "* (done) Metric: kendall tau distance\n",
    "* (done) wall-clock running time\n",
    "* (done) test step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyterhub",
   "language": "python",
   "name": "jupyterhub"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
