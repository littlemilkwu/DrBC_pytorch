{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch_geometric\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import read_test1_data\n",
    "from utils import gen_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 11\n",
    "\n",
    "SYNTHETIC_NUM = 3000\n",
    "# SYNTHETIC_NUM = 100\n",
    "\n",
    "# number of gen nodes\n",
    "NUM_MIN = 100\n",
    "NUM_MAX = 200\n",
    "\n",
    "LEARNING_RATE = 1e-4\n",
    "EMBEDDING_SIZE = 128\n",
    "DEPTH = 5\n",
    "BATCH_SIZE = 16\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1_X, test1_bc = read_test1_data(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Synthetic Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "416\n"
     ]
    }
   ],
   "source": [
    "train_g = gen_graph(NUM_MIN, NUM_MAX)\n",
    "print(len(train_g.edges()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[100, 104],\n",
       "       [100, 105],\n",
       "       [100, 107],\n",
       "       [100, 134],\n",
       "       [100, 162],\n",
       "       [100, 168],\n",
       "       [100, 176],\n",
       "       [100, 194],\n",
       "       [100, 200],\n",
       "       [101, 104]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.array(list(train_g.edges())) + 100)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 5, 7, 34, 62, 68, 76, 94, 100]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_g.neighbors(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = []\n",
    "for node in list(train_g.nodes())[:5]:\n",
    "    ls.append(list(train_g.neighbors(node)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nx.betweenness_centrality(train_g)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DrBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Module, Linear, Parameter, GRUCell, Sequential, ReLU, functional as t_F\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_synthetic():\n",
    "    g_list = []\n",
    "    dg_list = []\n",
    "    bc_list = []\n",
    "    for i in range(SYNTHETIC_NUM):\n",
    "        g = gen_graph(NUM_MIN, NUM_MAX)\n",
    "        g_list.append(g)\n",
    "        dg_list.append(list(dict(nx.degree(g)).values()))\n",
    "        bc_list.append(list(nx.betweenness_centrality(g)))\n",
    "        \n",
    "    return g_list, dg_list, bc_list\n",
    "\n",
    "def preprocessing_data(train_g:list, train_dg:list, train_bc:list):\n",
    "    X = np.zeros(shape=(0, 3))\n",
    "    y = np.zeros(shape=(0, ))\n",
    "    edge_index = np.zeros(shape=(0, 2))\n",
    "    pre_index = 0\n",
    "    for i in range(len(train_bc)):\n",
    "        assert len(train_dg[i]) == len(train_bc[i]) == len(train_g[i].nodes())\n",
    "        # make suer is has same nodes number.\n",
    "        num_node = len(train_dg[i])\n",
    "        _X = np.expand_dims(np.array(train_dg[i]), axis=-1)\n",
    "        _it = np.ones(shape=(_X.shape[0], 2))\n",
    "        _X = np.hstack([_X, _it])\n",
    "        X = np.append(X, _X, axis=0)\n",
    "\n",
    "        _y = np.array(train_bc[i])\n",
    "        y = np.append(y, _y, axis=0)\n",
    "\n",
    "        _edge = np.array(list(train_g[i].edges())) + pre_index\n",
    "        edge_index = np.append(edge_index, _edge, axis=0)\n",
    "\n",
    "        pre_index += num_node\n",
    "    X = torch.Tensor(X)\n",
    "    y = torch.Tensor(y)\n",
    "    edge_index = torch.Tensor(edge_index).T.to(torch.int64)\n",
    "    # print(X.shape, y.shape, edge_index.shape)\n",
    "\n",
    "    return X, y, edge_index\n",
    "\n",
    "def get_pairwise_ids(g_list):\n",
    "    s_ids = np.zeros(shape=(0, ), dtype=int)\n",
    "    t_ids = np.zeros(shape=(0, ), dtype=int)\n",
    "    pre_index = 0\n",
    "    for g in g_list:\n",
    "        num_node = len(g.nodes())\n",
    "        ids_1 = np.repeat(np.arange(pre_index, pre_index+num_node), 5)\n",
    "        ids_2 = np.repeat(np.arange(pre_index, pre_index+num_node), 5)\n",
    "\n",
    "        np.random.shuffle(ids_1)\n",
    "        np.random.shuffle(ids_2)\n",
    "\n",
    "        s_ids = np.append(s_ids, ids_1, axis=0)\n",
    "        t_ids = np.append(t_ids, ids_2, axis=0)\n",
    "        pre_index += num_node\n",
    "    return s_ids, t_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNConv(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GCNConv, self).__init__(aggr='add')\n",
    "        self.lin = Linear(in_channels, out_channels, bias=False)\n",
    "        self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.lin.reset_parameters()\n",
    "        self.bias.data.zero_()\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        # edge_index, _ = add_self_loops(edge_index, num_nodes=x.shape[0])\n",
    "        # x = self.lin(x)\n",
    "\n",
    "        row, col = edge_index\n",
    "        rc = torch.cat([row, col], axis=0)\n",
    "        deg = degree(rc, x.shape[0], dtype=x.dtype)\n",
    "        deg += 1\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "\n",
    "        out = self.propagate(edge_index, x=x, norm=norm)\n",
    "        # out += self.bias\n",
    "        return out\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        return norm.view(-1, 1) * x_j\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DrBC(Module):\n",
    "    def __init__(self, embedding_size=EMBEDDING_SIZE, depth=DEPTH):\n",
    "        super(DrBC, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.depth = depth\n",
    "        self.linear0 = Linear(3, self.embedding_size)\n",
    "        self.gcn = GCNConv(self.embedding_size, self.embedding_size)\n",
    "        self.gru = GRUCell(self.embedding_size, self.embedding_size)\n",
    "        # decoder\n",
    "        self.mlp = Sequential(\n",
    "            Linear(self.embedding_size, self.embedding_size // 2),\n",
    "            ReLU(),\n",
    "            Linear(self.embedding_size // 2, 1)\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, X, edge_index):\n",
    "        all_h = []\n",
    "        h = self.linear0(X)\n",
    "        h = torch.relu(h)\n",
    "        h = t_F.normalize(h, p=2, dim=-1) # l2-norm\n",
    "        all_h.append(torch.unsqueeze(h, dim=0))\n",
    "        # GRUCell\n",
    "        for i in range(self.depth-1):\n",
    "            # neighborhood aggregation\n",
    "            h_aggre = self.gcn(h, edge_index)\n",
    "            h = self.gru(h_aggre, h)\n",
    "            h = t_F.normalize(h, p=2, dim=-1) # l2-norm\n",
    "            all_h.append(torch.unsqueeze(h, dim=0))\n",
    "        \n",
    "        # max pooling\n",
    "        all_h = torch.cat(all_h, dim=0)\n",
    "        h_max = torch.max(all_h, dim=0).values\n",
    "        # print('h_max shape: ', h_max.shape)\n",
    "\n",
    "        # Decoder\n",
    "        out = self.mlp(h_max)\n",
    "        out = torch.squeeze(out)\n",
    "        # print('out shape: ', out.shape)\n",
    "        return out\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "model = DrBC().to(device)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of DrBC(\n",
       "  (linear0): Linear(in_features=3, out_features=128, bias=True)\n",
       "  (gcn): GCNConv()\n",
       "  (gru): GRUCell(128, 128)\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pm0 shape: torch.Size([128, 3])\n",
      "pm1 shape: torch.Size([128])\n",
      "pm2 shape: torch.Size([128])\n",
      "pm3 shape: torch.Size([128, 128])\n",
      "pm4 shape: torch.Size([384, 128])\n",
      "pm5 shape: torch.Size([384, 128])\n",
      "pm6 shape: torch.Size([384])\n",
      "pm7 shape: torch.Size([384])\n",
      "pm8 shape: torch.Size([64, 128])\n",
      "pm9 shape: torch.Size([64])\n",
      "pm10 shape: torch.Size([1, 64])\n",
      "pm11 shape: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "pm = list(model.parameters())\n",
    "\n",
    "pm[0].shape # W0\n",
    "pm[1].shape # gru\n",
    "pm[2].shape # 384, 128\n",
    "pm[3].shape # 384, 128\n",
    "pm[4].shape # 384\n",
    "pm[5].shape # 384\n",
    "\n",
    "for i, p in enumerate(pm):\n",
    "    print(f\"pm{i} shape: {p.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- prepare systhetic done\n",
      "Batch 1: Loss = 0.693223237991333\n",
      "Batch 2: Loss = 0.69290691614151\n",
      "Batch 3: Loss = 0.6925891041755676\n",
      "Batch 4: Loss = 0.6922805309295654\n",
      "Batch 5: Loss = 0.6919754147529602\n",
      "Batch 6: Loss = 0.6917043924331665\n",
      "Batch 7: Loss = 0.6914026737213135\n",
      "Batch 8: Loss = 0.6911582946777344\n",
      "Batch 9: Loss = 0.6909435391426086\n",
      "Batch 10: Loss = 0.6906291246414185\n",
      "Batch 11: Loss = 0.6903756260871887\n",
      "Batch 12: Loss = 0.6901136040687561\n",
      "Batch 13: Loss = 0.68986976146698\n",
      "Batch 14: Loss = 0.6895608305931091\n",
      "Batch 15: Loss = 0.6893821358680725\n",
      "Batch 16: Loss = 0.6890992522239685\n",
      "Batch 17: Loss = 0.6888858079910278\n",
      "Batch 18: Loss = 0.6886112093925476\n",
      "Batch 19: Loss = 0.688306450843811\n",
      "Batch 20: Loss = 0.6880044937133789\n",
      "Batch 21: Loss = 0.6877583861351013\n",
      "Batch 22: Loss = 0.6875669956207275\n",
      "Batch 23: Loss = 0.6873530149459839\n",
      "Batch 24: Loss = 0.6869648098945618\n",
      "Batch 25: Loss = 0.6868781447410583\n",
      "Batch 26: Loss = 0.6865797638893127\n",
      "Batch 27: Loss = 0.6862043142318726\n",
      "Batch 28: Loss = 0.686009407043457\n",
      "Batch 29: Loss = 0.6855591535568237\n",
      "Batch 30: Loss = 0.6853218078613281\n",
      "Batch 31: Loss = 0.6850203275680542\n",
      "Batch 32: Loss = 0.684837281703949\n",
      "Batch 33: Loss = 0.6845159530639648\n",
      "Batch 34: Loss = 0.6841872930526733\n",
      "Batch 35: Loss = 0.6836768388748169\n",
      "Batch 36: Loss = 0.683458149433136\n",
      "Batch 37: Loss = 0.6830621361732483\n",
      "Batch 38: Loss = 0.6827031970024109\n",
      "Batch 39: Loss = 0.6824687719345093\n",
      "Batch 40: Loss = 0.6815920472145081\n",
      "Batch 41: Loss = 0.6815290451049805\n",
      "Batch 42: Loss = 0.6812518835067749\n",
      "Batch 43: Loss = 0.6807274222373962\n",
      "Batch 44: Loss = 0.6802106499671936\n",
      "Batch 45: Loss = 0.6796255707740784\n",
      "Batch 46: Loss = 0.6792982220649719\n",
      "Batch 47: Loss = 0.6786921620368958\n",
      "Batch 48: Loss = 0.6784130334854126\n",
      "Batch 49: Loss = 0.6776646375656128\n",
      "Batch 50: Loss = 0.6770938634872437\n",
      "Batch 51: Loss = 0.67652827501297\n",
      "Batch 52: Loss = 0.6761832237243652\n",
      "Batch 53: Loss = 0.6752099394798279\n",
      "Batch 54: Loss = 0.6755108833312988\n",
      "Batch 55: Loss = 0.6744363903999329\n",
      "Batch 56: Loss = 0.6737834215164185\n",
      "Batch 57: Loss = 0.6730741858482361\n",
      "Batch 58: Loss = 0.6725230813026428\n",
      "Batch 59: Loss = 0.6718484163284302\n",
      "Batch 60: Loss = 0.6710972189903259\n",
      "Batch 61: Loss = 0.6705479025840759\n",
      "Batch 62: Loss = 0.6696105599403381\n",
      "Batch 63: Loss = 0.668854296207428\n",
      "Batch 64: Loss = 0.667618453502655\n",
      "Batch 65: Loss = 0.6665098071098328\n",
      "Batch 66: Loss = 0.6659491062164307\n",
      "Batch 67: Loss = 0.6657120585441589\n",
      "Batch 68: Loss = 0.6641612648963928\n",
      "Batch 69: Loss = 0.6631741523742676\n",
      "Batch 70: Loss = 0.6622020602226257\n",
      "Batch 71: Loss = 0.6610385775566101\n",
      "Batch 72: Loss = 0.6605120897293091\n",
      "Batch 73: Loss = 0.6592046618461609\n",
      "Batch 74: Loss = 0.6582653522491455\n",
      "Batch 75: Loss = 0.6569210290908813\n",
      "Batch 76: Loss = 0.6559812426567078\n",
      "Batch 77: Loss = 0.6546491384506226\n",
      "Batch 78: Loss = 0.6530227065086365\n",
      "Batch 79: Loss = 0.6519274711608887\n",
      "Batch 80: Loss = 0.6509253978729248\n",
      "Batch 81: Loss = 0.6498687267303467\n",
      "Batch 82: Loss = 0.648048460483551\n",
      "Batch 83: Loss = 0.6466893553733826\n",
      "Batch 84: Loss = 0.64691561460495\n",
      "Batch 85: Loss = 0.6445069313049316\n",
      "Batch 86: Loss = 0.6418680548667908\n",
      "Batch 87: Loss = 0.6402177810668945\n",
      "Batch 88: Loss = 0.640681803226471\n",
      "Batch 89: Loss = 0.6382431387901306\n",
      "Batch 90: Loss = 0.636795163154602\n",
      "Batch 91: Loss = 0.6350621581077576\n",
      "Batch 92: Loss = 0.6343399286270142\n",
      "Batch 93: Loss = 0.6328457593917847\n",
      "Batch 94: Loss = 0.6298772692680359\n",
      "Batch 95: Loss = 0.6284180879592896\n",
      "Batch 96: Loss = 0.6276395916938782\n",
      "Batch 97: Loss = 0.6266472339630127\n",
      "Batch 98: Loss = 0.6253668665885925\n",
      "Batch 99: Loss = 0.6231822371482849\n",
      "Batch 100: Loss = 0.622858464717865\n",
      "Batch 101: Loss = 0.6209350228309631\n",
      "Batch 102: Loss = 0.6185817718505859\n",
      "Batch 103: Loss = 0.6160642504692078\n",
      "Batch 104: Loss = 0.6154685020446777\n",
      "Batch 105: Loss = 0.6131433844566345\n",
      "Batch 106: Loss = 0.6105117201805115\n",
      "Batch 107: Loss = 0.610075831413269\n",
      "Batch 108: Loss = 0.6090990304946899\n",
      "Batch 109: Loss = 0.6070795655250549\n",
      "Batch 110: Loss = 0.6054375171661377\n",
      "Batch 111: Loss = 0.6029366254806519\n",
      "Batch 112: Loss = 0.6029205918312073\n",
      "Batch 113: Loss = 0.6000174283981323\n",
      "Batch 114: Loss = 0.599747896194458\n",
      "Batch 115: Loss = 0.5973608493804932\n",
      "Batch 116: Loss = 0.5985891222953796\n",
      "Batch 117: Loss = 0.5959222912788391\n",
      "Batch 118: Loss = 0.5942292213439941\n",
      "Batch 119: Loss = 0.5919453501701355\n",
      "Batch 120: Loss = 0.5895815491676331\n",
      "Batch 121: Loss = 0.5891515016555786\n",
      "Batch 122: Loss = 0.5875150561332703\n",
      "Batch 123: Loss = 0.5858243107795715\n",
      "Batch 124: Loss = 0.5853515863418579\n",
      "Batch 125: Loss = 0.583954393863678\n",
      "Batch 126: Loss = 0.5833677053451538\n",
      "Batch 127: Loss = 0.5808302164077759\n",
      "Batch 128: Loss = 0.5799306035041809\n",
      "Batch 129: Loss = 0.5769279599189758\n",
      "Batch 130: Loss = 0.5786769986152649\n",
      "Batch 131: Loss = 0.5784499645233154\n",
      "Batch 132: Loss = 0.5742175579071045\n",
      "Batch 133: Loss = 0.5705591440200806\n",
      "Batch 134: Loss = 0.5715780258178711\n",
      "Batch 135: Loss = 0.5729528665542603\n",
      "Batch 136: Loss = 0.5707817077636719\n",
      "Batch 137: Loss = 0.566338062286377\n",
      "Batch 138: Loss = 0.5674809217453003\n",
      "Batch 139: Loss = 0.5670695304870605\n",
      "Batch 140: Loss = 0.5632163882255554\n",
      "Batch 141: Loss = 0.5612977147102356\n",
      "Batch 142: Loss = 0.5595592856407166\n",
      "Batch 143: Loss = 0.5577794313430786\n",
      "Batch 144: Loss = 0.5581609010696411\n",
      "Batch 145: Loss = 0.5576059222221375\n",
      "Batch 146: Loss = 0.5553610920906067\n",
      "Batch 147: Loss = 0.5598935484886169\n",
      "Batch 148: Loss = 0.553511917591095\n",
      "Batch 149: Loss = 0.5523347854614258\n",
      "Batch 150: Loss = 0.5496807098388672\n",
      "Batch 151: Loss = 0.5522318482398987\n",
      "Batch 152: Loss = 0.548385500907898\n",
      "Batch 153: Loss = 0.5462561249732971\n",
      "Batch 154: Loss = 0.5480826497077942\n",
      "Batch 155: Loss = 0.5419342517852783\n",
      "Batch 156: Loss = 0.5433328151702881\n",
      "Batch 157: Loss = 0.5456098318099976\n",
      "Batch 158: Loss = 0.5413780808448792\n",
      "Batch 159: Loss = 0.5391359329223633\n",
      "Batch 160: Loss = 0.5431930422782898\n",
      "Batch 161: Loss = 0.5365649461746216\n",
      "Batch 162: Loss = 0.5365907549858093\n",
      "Batch 163: Loss = 0.5361597537994385\n",
      "Batch 164: Loss = 0.5350857377052307\n",
      "Batch 165: Loss = 0.5342035293579102\n",
      "Batch 166: Loss = 0.5323392748832703\n",
      "Batch 167: Loss = 0.5295569896697998\n",
      "Batch 168: Loss = 0.529568076133728\n",
      "Batch 169: Loss = 0.5294170379638672\n",
      "Batch 170: Loss = 0.5306259989738464\n",
      "Batch 171: Loss = 0.5281444787979126\n",
      "Batch 172: Loss = 0.5299822688102722\n",
      "Batch 173: Loss = 0.5264750719070435\n",
      "Batch 174: Loss = 0.5199964046478271\n",
      "Batch 175: Loss = 0.5219916701316833\n",
      "Batch 176: Loss = 0.5201702117919922\n",
      "Batch 177: Loss = 0.5228137969970703\n",
      "Batch 178: Loss = 0.5148466229438782\n",
      "Batch 179: Loss = 0.5157628655433655\n",
      "Batch 180: Loss = 0.5176675915718079\n",
      "Batch 181: Loss = 0.5142515897750854\n",
      "Batch 182: Loss = 0.5109995603561401\n",
      "Batch 183: Loss = 0.5125301480293274\n",
      "Batch 184: Loss = 0.5108250975608826\n",
      "Batch 185: Loss = 0.5105490684509277\n",
      "Batch 186: Loss = 0.5067347288131714\n",
      "Batch 187: Loss = 0.5056865215301514\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    g_list, dg_list, bc_list  = prepare_synthetic()\n",
    "    print('-'*20, 'prepare systhetic done')\n",
    "    batch_cnt = len(g_list) // BATCH_SIZE\n",
    "\n",
    "    for i in range(batch_cnt):\n",
    "        s_index = i*BATCH_SIZE\n",
    "        e_index = (i+1)*BATCH_SIZE\n",
    "        train_g, train_dg, train_bc = g_list[s_index: e_index], dg_list[s_index: e_index], bc_list[s_index: e_index]\n",
    "        X, y, edge_index = preprocessing_data(train_g, train_dg, train_bc)\n",
    "        X, y, edge_index = X.to(device), y.to(device), edge_index.to(device)\n",
    "        out = model(X, edge_index)\n",
    "\n",
    "        # pairwise-loss\n",
    "        s_ids, t_ids = get_pairwise_ids(train_g)\n",
    "        out_diff = out[s_ids] - out[t_ids]\n",
    "        y_diff = y[s_ids] - y[t_ids]\n",
    "        loss = loss_fn(out_diff, torch.sigmoid(y_diff))\n",
    "\n",
    "        # optim\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        print(f\"Batch {i + 1}: Loss = {loss.item()}\")\n",
    "        # print('pm0: ', list(model.parameters())[0])\n",
    "        \n",
    "def validate():\n",
    "    pass\n",
    "\n",
    "_ = train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To-Do List\n",
    "* (done) loss_fn 再加上 sigmoid\n",
    "* (done) pairwise 目前跨圖了\n",
    "* (done) h 要 normalized\n",
    "* (doen) aggregate 改成 MessagePassing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyterhub",
   "language": "python",
   "name": "jupyterhub"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
