{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch_geometric\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import read_test1_data\n",
    "from utils import gen_graph\n",
    "from utils import prepare_synthetic\n",
    "from utils import shuffle_graph\n",
    "from utils import preprocessing_data\n",
    "from utils import get_pairwise_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 11\n",
    "SYNTHETIC_NUM = 1000\n",
    "\n",
    "# number of gen nodes\n",
    "NUM_MIN = 100\n",
    "NUM_MAX = 200\n",
    "\n",
    "MAX_EPOCHS = 10000\n",
    "LEARNING_RATE = 1e-4\n",
    "EMBEDDING_SIZE = 128\n",
    "DEPTH = 5\n",
    "BATCH_SIZE = 16\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1_X, test1_bc = read_test1_data(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Synthetic Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "659\n"
     ]
    }
   ],
   "source": [
    "train_g = gen_graph(NUM_MIN, NUM_MAX)\n",
    "print(len(train_g.edges()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[100, 104],\n",
       "       [100, 108],\n",
       "       [100, 137],\n",
       "       [100, 153],\n",
       "       [101, 104],\n",
       "       [101, 105],\n",
       "       [101, 106],\n",
       "       [101, 107],\n",
       "       [101, 109],\n",
       "       [101, 110]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.array(list(train_g.edges())) + 100)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nx.betweenness_centrality(train_g)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DrBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Module, Linear, Parameter, GRUCell, Sequential, ReLU, functional as t_F\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCNConv(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(GCNConv, self).__init__(aggr='add')\n",
    "        self.lin = Linear(in_channels, out_channels, bias=False)\n",
    "        self.bias = Parameter(torch.Tensor(out_channels))\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.lin.reset_parameters()\n",
    "        self.bias.data.zero_()\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        row, col = edge_index\n",
    "        rc = torch.cat([row, col], axis=0)\n",
    "        deg = degree(rc, x.shape[0], dtype=x.dtype)\n",
    "        deg += 1\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "\n",
    "        out = self.propagate(edge_index, x=x, norm=norm)\n",
    "        return out\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        return norm.view(-1, 1) * x_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DrBC(Module):\n",
    "    def __init__(self, embedding_size=EMBEDDING_SIZE, depth=DEPTH):\n",
    "        super(DrBC, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.depth = depth\n",
    "        self.linear0 = Linear(3, self.embedding_size)\n",
    "        self.gcn = GCNConv(self.embedding_size, self.embedding_size)\n",
    "        self.gru = GRUCell(self.embedding_size, self.embedding_size)\n",
    "        # decoder\n",
    "        self.mlp = Sequential(\n",
    "            Linear(self.embedding_size, self.embedding_size // 2),\n",
    "            ReLU(),\n",
    "            Linear(self.embedding_size // 2, 1)\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, X, edge_index):\n",
    "        all_h = []\n",
    "        h = self.linear0(X)\n",
    "        h = torch.relu(h)\n",
    "        h = t_F.normalize(h, p=2, dim=-1) # l2-norm\n",
    "        all_h.append(torch.unsqueeze(h, dim=0))\n",
    "        # GRUCell\n",
    "        for i in range(self.depth-1):\n",
    "            # neighborhood aggregation\n",
    "            h_aggre = self.gcn(h, edge_index)\n",
    "            h = self.gru(h_aggre, h)\n",
    "            h = t_F.normalize(h, p=2, dim=-1) # l2-norm\n",
    "            all_h.append(torch.unsqueeze(h, dim=0))\n",
    "        \n",
    "        # max pooling\n",
    "        all_h = torch.cat(all_h, dim=0)\n",
    "        h_max = torch.max(all_h, dim=0).values\n",
    "        # print('h_max shape: ', h_max.shape)\n",
    "\n",
    "        # Decoder\n",
    "        out = self.mlp(h_max)\n",
    "        out = torch.squeeze(out)\n",
    "        # print('out shape: ', out.shape)\n",
    "        return out\n",
    "        \n",
    "        \n",
    "model = DrBC().to(device)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of DrBC(\n",
       "  (linear0): Linear(in_features=3, out_features=128, bias=True)\n",
       "  (gcn): GCNConv()\n",
       "  (gru): GRUCell(128, 128)\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pm0 shape: torch.Size([128, 3])\n",
      "pm1 shape: torch.Size([128])\n",
      "pm2 shape: torch.Size([128])\n",
      "pm3 shape: torch.Size([128, 128])\n",
      "pm4 shape: torch.Size([384, 128])\n",
      "pm5 shape: torch.Size([384, 128])\n",
      "pm6 shape: torch.Size([384])\n",
      "pm7 shape: torch.Size([384])\n",
      "pm8 shape: torch.Size([64, 128])\n",
      "pm9 shape: torch.Size([64])\n",
      "pm10 shape: torch.Size([1, 64])\n",
      "pm11 shape: torch.Size([1])\n"
     ]
    }
   ],
   "source": [
    "pm = list(model.parameters())\n",
    "\n",
    "for i, p in enumerate(pm):\n",
    "    print(f\"pm{i} shape: {p.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- prepare systhetic done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs 1    : : 100%|██████████| 62/62 [00:01<00:00, 50.53it/s, loss=0.685]\n",
      "Epochs 2    : : 100%|██████████| 62/62 [00:01<00:00, 49.94it/s, loss=0.636]\n",
      "Epochs 3    : : 100%|██████████| 62/62 [00:01<00:00, 53.72it/s, loss=0.559]\n",
      "Epochs 4    : : 100%|██████████| 62/62 [00:01<00:00, 52.35it/s, loss=0.496]\n",
      "Epochs 5    : : 100%|██████████| 62/62 [00:01<00:00, 52.11it/s, loss=0.448]\n",
      "Epochs 6    : : 100%|██████████| 62/62 [00:01<00:00, 51.45it/s, loss=0.416]\n",
      "Epochs 7    : : 100%|██████████| 62/62 [00:01<00:00, 51.08it/s, loss=0.394]\n",
      "Epochs 8    : : 100%|██████████| 62/62 [00:01<00:00, 51.39it/s, loss=0.379]\n",
      "Epochs 9    : : 100%|██████████| 62/62 [00:01<00:00, 54.09it/s, loss=0.367]\n",
      "Epochs 10   : : 100%|██████████| 62/62 [00:01<00:00, 55.17it/s, loss=0.358]\n",
      "Epochs 11   : : 100%|██████████| 62/62 [00:01<00:00, 51.68it/s, loss=0.351]\n",
      "Epochs 12   : : 100%|██████████| 62/62 [00:01<00:00, 48.41it/s, loss=0.346]\n",
      "Epochs 13   : : 100%|██████████| 62/62 [00:01<00:00, 47.62it/s, loss=0.343]\n",
      "Epochs 14   : : 100%|██████████| 62/62 [00:01<00:00, 48.80it/s, loss=0.339]\n",
      "Epochs 15   : : 100%|██████████| 62/62 [00:01<00:00, 47.75it/s, loss=0.336]\n",
      "Epochs 16   : : 100%|██████████| 62/62 [00:01<00:00, 48.87it/s, loss=0.333]\n",
      "Epochs 17   : : 100%|██████████| 62/62 [00:01<00:00, 48.95it/s, loss=0.332]\n",
      "Epochs 18   : : 100%|██████████| 62/62 [00:01<00:00, 48.90it/s, loss=0.331]\n",
      "Epochs 19   : : 100%|██████████| 62/62 [00:01<00:00, 49.45it/s, loss=0.331]\n",
      "Epochs 20   : : 100%|██████████| 62/62 [00:01<00:00, 51.81it/s, loss=0.328]\n",
      "Epochs 21   : : 100%|██████████| 62/62 [00:01<00:00, 51.19it/s, loss=0.328]\n",
      "Epochs 22   : : 100%|██████████| 62/62 [00:01<00:00, 51.57it/s, loss=0.327]\n",
      "Epochs 23   : : 100%|██████████| 62/62 [00:01<00:00, 46.39it/s, loss=0.326]\n",
      "Epochs 24   : : 100%|██████████| 62/62 [00:01<00:00, 48.13it/s, loss=0.326]\n",
      "Epochs 25   : : 100%|██████████| 62/62 [00:01<00:00, 47.67it/s, loss=0.326]\n",
      "Epochs 26   : : 100%|██████████| 62/62 [00:01<00:00, 51.60it/s, loss=0.325]\n",
      "Epochs 27   : : 100%|██████████| 62/62 [00:01<00:00, 51.53it/s, loss=0.324]\n",
      "Epochs 28   : : 100%|██████████| 62/62 [00:01<00:00, 50.88it/s, loss=0.324]\n",
      "Epochs 29   : : 100%|██████████| 62/62 [00:01<00:00, 51.79it/s, loss=0.324]\n",
      "Epochs 30   : : 100%|██████████| 62/62 [00:01<00:00, 51.12it/s, loss=0.323]\n",
      "Epochs 31   : : 100%|██████████| 62/62 [00:01<00:00, 51.71it/s, loss=0.323]\n",
      "Epochs 32   : : 100%|██████████| 62/62 [00:01<00:00, 51.88it/s, loss=0.322]\n",
      "Epochs 33   : : 100%|██████████| 62/62 [00:01<00:00, 52.02it/s, loss=0.324]\n",
      "Epochs 34   : : 100%|██████████| 62/62 [00:01<00:00, 52.06it/s, loss=0.322]\n",
      "Epochs 35   : : 100%|██████████| 62/62 [00:01<00:00, 49.81it/s, loss=0.322]\n",
      "Epochs 36   : : 100%|██████████| 62/62 [00:01<00:00, 51.28it/s, loss=0.322]\n",
      "Epochs 37   : : 100%|██████████| 62/62 [00:01<00:00, 51.29it/s, loss=0.321]\n",
      "Epochs 38   : : 100%|██████████| 62/62 [00:01<00:00, 49.26it/s, loss=0.321]\n",
      "Epochs 39   : : 100%|██████████| 62/62 [00:01<00:00, 50.98it/s, loss=0.322]\n",
      "Epochs 40   : : 100%|██████████| 62/62 [00:01<00:00, 48.17it/s, loss=0.321]\n",
      "Epochs 41   : : 100%|██████████| 62/62 [00:01<00:00, 48.16it/s, loss=0.322]\n",
      "Epochs 42   : : 100%|██████████| 62/62 [00:01<00:00, 47.47it/s, loss=0.32]\n",
      "Epochs 43   : : 100%|██████████| 62/62 [00:01<00:00, 48.65it/s, loss=0.321]\n",
      "Epochs 44   : : 100%|██████████| 62/62 [00:01<00:00, 48.53it/s, loss=0.32]\n",
      "Epochs 45   : : 100%|██████████| 62/62 [00:01<00:00, 47.68it/s, loss=0.32]\n",
      "Epochs 46   : : 100%|██████████| 62/62 [00:01<00:00, 50.62it/s, loss=0.319]\n",
      "Epochs 47   : : 100%|██████████| 62/62 [00:01<00:00, 51.44it/s, loss=0.32]\n",
      "Epochs 48   : : 100%|██████████| 62/62 [00:01<00:00, 51.97it/s, loss=0.319]\n",
      "Epochs 49   : : 100%|██████████| 62/62 [00:01<00:00, 55.07it/s, loss=0.319]\n",
      "Epochs 50   : : 100%|██████████| 62/62 [00:01<00:00, 49.46it/s, loss=0.319]\n",
      "Epochs 51   : : 100%|██████████| 62/62 [00:01<00:00, 48.22it/s, loss=0.321]\n",
      "Epochs 52   : : 100%|██████████| 62/62 [00:01<00:00, 51.58it/s, loss=0.319]\n",
      "Epochs 53   : : 100%|██████████| 62/62 [00:01<00:00, 49.49it/s, loss=0.319]\n",
      "Epochs 54   : : 100%|██████████| 62/62 [00:01<00:00, 50.96it/s, loss=0.319]\n",
      "Epochs 55   : : 100%|██████████| 62/62 [00:01<00:00, 53.65it/s, loss=0.319]\n",
      "Epochs 56   : : 100%|██████████| 62/62 [00:01<00:00, 51.75it/s, loss=0.32]\n",
      "Epochs 57   : : 100%|██████████| 62/62 [00:01<00:00, 50.70it/s, loss=0.319]\n",
      "Epochs 58   : : 100%|██████████| 62/62 [00:01<00:00, 52.79it/s, loss=0.319]\n",
      "Epochs 59   : : 100%|██████████| 62/62 [00:01<00:00, 52.34it/s, loss=0.318]\n",
      "Epochs 60   : : 100%|██████████| 62/62 [00:01<00:00, 48.26it/s, loss=0.32]\n",
      "Epochs 61   : : 100%|██████████| 62/62 [00:01<00:00, 48.38it/s, loss=0.319]\n",
      "Epochs 62   : : 100%|██████████| 62/62 [00:01<00:00, 48.02it/s, loss=0.319]\n",
      "Epochs 63   : : 100%|██████████| 62/62 [00:01<00:00, 47.71it/s, loss=0.319]\n",
      "Epochs 64   : : 100%|██████████| 62/62 [00:01<00:00, 47.30it/s, loss=0.319]\n",
      "Epochs 65   : : 100%|██████████| 62/62 [00:01<00:00, 52.89it/s, loss=0.32]\n",
      "Epochs 66   : : 100%|██████████| 62/62 [00:01<00:00, 49.63it/s, loss=0.319]\n",
      "Epochs 67   : : 100%|██████████| 62/62 [00:01<00:00, 49.60it/s, loss=0.318]\n",
      "Epochs 68   : : 100%|██████████| 62/62 [00:01<00:00, 49.26it/s, loss=0.319]\n",
      "Epochs 69   : : 100%|██████████| 62/62 [00:01<00:00, 50.01it/s, loss=0.318]\n",
      "Epochs 70   : : 100%|██████████| 62/62 [00:01<00:00, 53.49it/s, loss=0.318]\n",
      "Epochs 71   : : 100%|██████████| 62/62 [00:01<00:00, 57.45it/s, loss=0.318]\n",
      "Epochs 72   : : 100%|██████████| 62/62 [00:01<00:00, 53.38it/s, loss=0.319]\n",
      "Epochs 73   : : 100%|██████████| 62/62 [00:01<00:00, 51.11it/s, loss=0.318]\n",
      "Epochs 74   : : 100%|██████████| 62/62 [00:01<00:00, 51.44it/s, loss=0.319]\n",
      "Epochs 75   : : 100%|██████████| 62/62 [00:01<00:00, 52.20it/s, loss=0.319]\n",
      "Epochs 76   : : 100%|██████████| 62/62 [00:01<00:00, 48.18it/s, loss=0.318]\n",
      "Epochs 77   : : 100%|██████████| 62/62 [00:01<00:00, 47.90it/s, loss=0.319]\n",
      "Epochs 78   : : 100%|██████████| 62/62 [00:01<00:00, 44.55it/s, loss=0.318]\n",
      "Epochs 79   : : 100%|██████████| 62/62 [00:01<00:00, 45.88it/s, loss=0.317]\n",
      "Epochs 80   : : 100%|██████████| 62/62 [00:01<00:00, 47.50it/s, loss=0.318]\n",
      "Epochs 81   : : 100%|██████████| 62/62 [00:01<00:00, 46.32it/s, loss=0.318]\n",
      "Epochs 82   : : 100%|██████████| 62/62 [00:01<00:00, 47.47it/s, loss=0.319]\n",
      "Epochs 83   : : 100%|██████████| 62/62 [00:01<00:00, 47.56it/s, loss=0.318]\n",
      "Epochs 84   : : 100%|██████████| 62/62 [00:01<00:00, 51.79it/s, loss=0.318]\n",
      "Epochs 85   : : 100%|██████████| 62/62 [00:01<00:00, 48.46it/s, loss=0.318]\n",
      "Epochs 86   : : 100%|██████████| 62/62 [00:01<00:00, 47.65it/s, loss=0.319]\n",
      "Epochs 87   : : 100%|██████████| 62/62 [00:01<00:00, 48.09it/s, loss=0.319]\n",
      "Epochs 88   : : 100%|██████████| 62/62 [00:01<00:00, 48.12it/s, loss=0.317]\n",
      "Epochs 89   : : 100%|██████████| 62/62 [00:01<00:00, 48.20it/s, loss=0.319]\n",
      "Epochs 90   : : 100%|██████████| 62/62 [00:01<00:00, 47.84it/s, loss=0.318]\n",
      "Epochs 91   : : 100%|██████████| 62/62 [00:01<00:00, 48.16it/s, loss=0.319]\n",
      "Epochs 92   : : 100%|██████████| 62/62 [00:01<00:00, 48.94it/s, loss=0.318]\n",
      "Epochs 93   : : 100%|██████████| 62/62 [00:01<00:00, 48.94it/s, loss=0.319]\n",
      "Epochs 94   : : 100%|██████████| 62/62 [00:01<00:00, 47.16it/s, loss=0.318]\n",
      "Epochs 95   : : 100%|██████████| 62/62 [00:01<00:00, 48.86it/s, loss=0.318]\n",
      "Epochs 96   : : 100%|██████████| 62/62 [00:01<00:00, 50.11it/s, loss=0.317]\n",
      "Epochs 97   : : 100%|██████████| 62/62 [00:01<00:00, 50.78it/s, loss=0.318]\n",
      "Epochs 98   : : 100%|██████████| 62/62 [00:01<00:00, 51.28it/s, loss=0.318]\n",
      "Epochs 99   : : 100%|██████████| 62/62 [00:01<00:00, 51.34it/s, loss=0.318]\n",
      "Epochs 100  : : 100%|██████████| 62/62 [00:01<00:00, 51.98it/s, loss=0.318]\n"
     ]
    }
   ],
   "source": [
    "def train(model, optim, loss_fn, epochs:int):\n",
    "    g_list, dg_list, bc_list  = prepare_synthetic(SYNTHETIC_NUM, (NUM_MIN, NUM_MAX))\n",
    "    batch_cnt = len(g_list) // BATCH_SIZE\n",
    "    print('-'*20, 'prepare systhetic done')\n",
    "    for e in range(epochs):\n",
    "        g_list, dg_list, bc_list = shuffle_graph(g_list, dg_list, bc_list)\n",
    "        batch_bar = tqdm(range(batch_cnt))\n",
    "        batch_bar.set_description(f'Epochs {e+1:<5}: ')\n",
    "        train_loss = 0\n",
    "        pair_cnt = 0\n",
    "        for i in batch_bar:\n",
    "            # batch\n",
    "            s_index, e_index = i*BATCH_SIZE, (i+1)*BATCH_SIZE\n",
    "            train_g, train_dg, train_bc = g_list[s_index: e_index], dg_list[s_index: e_index], bc_list[s_index: e_index]\n",
    "            X, y, edge_index = preprocessing_data(train_g, train_dg, train_bc)\n",
    "            X, y, edge_index = X.to(device), y.to(device), edge_index.to(device)\n",
    "            out = model(X, edge_index)\n",
    "\n",
    "            # pairwise-loss\n",
    "            s_ids, t_ids = get_pairwise_ids(train_g)\n",
    "            out_diff = out[s_ids] - out[t_ids]\n",
    "            y_diff = y[s_ids] - y[t_ids]\n",
    "            loss = loss_fn(out_diff, torch.sigmoid(y_diff))\n",
    "\n",
    "            # optim\n",
    "            optim.zero_grad()\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "            pair_cnt += len(s_ids)\n",
    "            train_loss += loss.item() * len(s_ids)\n",
    "            if i == (batch_cnt - 1):\n",
    "                # last batch\n",
    "                train_loss /= pair_cnt\n",
    "                batch_bar.set_postfix(loss=train_loss)\n",
    "        \n",
    "def validate():\n",
    "    pass\n",
    "\n",
    "_ = train(model, optim, loss_fn, 100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To-Do List\n",
    "* (done) loss_fn 再加上 sigmoid\n",
    "* (done) pairwise 目前跨圖了\n",
    "* (done) h 要 normalized\n",
    "* (done) aggregate 改成 MessagePassing\n",
    "* (done) synthetic graph 後，shuffle graph 的順序\n",
    "* (done) 加入 Epochs\n",
    "* Metric: top1, 5, 10\n",
    "* Metric: kendall tau distance\n",
    "* wall-clock running time\n",
    "* test step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyterhub",
   "language": "python",
   "name": "jupyterhub"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
