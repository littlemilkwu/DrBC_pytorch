{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch_geometric\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import read_test1_data\n",
    "from utils import gen_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 11\n",
    "\n",
    "SYNTHETIC_NUM = 1000\n",
    "# SYNTHETIC_NUM = 100\n",
    "\n",
    "# number of gen nodes\n",
    "NUM_MIN = 100\n",
    "NUM_MAX = 200\n",
    "\n",
    "LEARNING_RATE = 1e-4\n",
    "EMBEDDING_SIZE = 128\n",
    "DEPTH = 5\n",
    "BATCH_SIZE = 4\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1_X, test1_bc = read_test1_data(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Synthetic Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "622\n"
     ]
    }
   ],
   "source": [
    "train_g = gen_graph(NUM_MIN, NUM_MAX)\n",
    "print(len(train_g.edges()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 8, 9, 16, 19, 81, 128, 142]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_g.neighbors(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = []\n",
    "for node in list(train_g.nodes())[:5]:\n",
    "    ls.append(list(train_g.neighbors(node)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nx.betweenness_centrality(train_g)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DrBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Module, Linear, GRUCell, Sequential, ReLU, functional as t_F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_synthetic():\n",
    "    g_list = []\n",
    "    dg_list = []\n",
    "    bc_list = []\n",
    "    for i in range(SYNTHETIC_NUM):\n",
    "        g = gen_graph(NUM_MIN, NUM_MAX)\n",
    "        g_list.append(g)\n",
    "        dg_list.append(nx.degree(g))\n",
    "        bc_list.append(nx.betweenness_centrality(g))\n",
    "        \n",
    "    return g_list, dg_list, bc_list\n",
    "\n",
    "def preprocessing_data(train_g:list, train_dg:list, train_bc:list):\n",
    "    X = []\n",
    "    y = []\n",
    "    nb = []\n",
    "    pre_index = 0\n",
    "    for i in range(len(train_bc)):\n",
    "        assert len(train_dg[i]) == len(train_bc[i]) == len(train_g[i].nodes())\n",
    "        # make suer is has same nodes number.\n",
    "        num_node = len(train_dg[i])\n",
    "        for node_id in range(num_node):\n",
    "            node_nb = [pre_index+n for n in train_g[i].neighbors(node_id)]\n",
    "            X.append([train_dg[i][node_id], 1., 1.])\n",
    "            y.append(train_bc[i][node_id])\n",
    "            nb.append(node_nb)\n",
    "        pre_index += num_node\n",
    "    X = torch.Tensor(X)\n",
    "    y = torch.Tensor(y)\n",
    "    # print(X.shape, y.shape)\n",
    "\n",
    "    return X, y, nb\n",
    "\n",
    "def get_pairwise_ids(g_list):\n",
    "    s_ids = np.zeros(shape=(0, ), dtype=int)\n",
    "    t_ids = np.zeros(shape=(0, ), dtype=int)\n",
    "    pre_index = 0\n",
    "    for g in g_list:\n",
    "        num_node = len(g.nodes())\n",
    "        ids_1 = np.repeat(np.arange(pre_index, pre_index+num_node), 5)\n",
    "        ids_2 = np.repeat(np.arange(pre_index, pre_index+num_node), 5)\n",
    "\n",
    "        np.random.shuffle(ids_1)\n",
    "        np.random.shuffle(ids_2)\n",
    "\n",
    "        s_ids = np.append(s_ids, ids_1, axis=0)\n",
    "        t_ids = np.append(t_ids, ids_2, axis=0)\n",
    "        pre_index += num_node\n",
    "    return s_ids, t_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DrBC(Module):\n",
    "    def __init__(self, embedding_size=EMBEDDING_SIZE, depth=DEPTH):\n",
    "        super(DrBC, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.depth = depth\n",
    "        self.linear0 = Linear(3, self.embedding_size)\n",
    "        self.gru = GRUCell(self.embedding_size, self.embedding_size)\n",
    "        # decoder\n",
    "        self.mlp = Sequential(\n",
    "            Linear(self.embedding_size, self.embedding_size // 2),\n",
    "            ReLU(),\n",
    "            Linear(self.embedding_size // 2, 1)\n",
    "        )\n",
    "        \n",
    "    def neighbor_aggre(self, X, all_nb, h):\n",
    "        # nb aggre\n",
    "        h_aggre = []\n",
    "        for node_id in range(X.shape[0]):\n",
    "            d_v = X[node_id, 0]\n",
    "            node_nb = all_nb[node_id]\n",
    "            node_aggre = torch.Tensor([0.] * self.embedding_size).to(device)\n",
    "            for nb_id in node_nb:\n",
    "                # for node all nb\n",
    "                node_aggre += (1 / (math.sqrt(d_v + 1) * math.sqrt(X[nb_id, 0] + 1))) * h[nb_id]\n",
    "            h_aggre.append(torch.unsqueeze(node_aggre, dim=0))\n",
    "        h_aggre = torch.cat(h_aggre, dim=0)\n",
    "        # print('h_aggre shape: ', h_aggre.shape)\n",
    "        return h_aggre # tensor format\n",
    "\n",
    "    def forward(self, X, all_nb):\n",
    "        all_h = []\n",
    "        h = self.linear0(X)\n",
    "        h = torch.relu(h)\n",
    "        h = t_F.normalize(h, p=2, dim=-1) # l2-norm\n",
    "        all_h.append(torch.unsqueeze(h, dim=0))\n",
    "\n",
    "        # GRUCell\n",
    "        for i in range(self.depth-1):\n",
    "            # neighborhood aggregation\n",
    "            h_aggre = self.neighbor_aggre(X, all_nb, h)\n",
    "            h = self.gru(h_aggre, h)\n",
    "            h = t_F.normalize(h, p=2, dim=-1) # l2-norm\n",
    "            all_h.append(torch.unsqueeze(h, dim=0))\n",
    "        \n",
    "        # max pooling\n",
    "        all_h = torch.cat(all_h, dim=0)\n",
    "        h_max = torch.max(all_h, dim=0).values\n",
    "        # print('h_max shape: ', h_max.shape)\n",
    "\n",
    "        # Decoder\n",
    "        out = self.mlp(h_max)\n",
    "        out = torch.squeeze(out)\n",
    "        # print('out shape: ', out.shape)\n",
    "        return out\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "model = DrBC().to(device)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of DrBC(\n",
       "  (linear0): Linear(in_features=3, out_features=128, bias=True)\n",
       "  (gru): GRUCell(128, 128)\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- prepare systhetic done\n",
      "Batch 1: Loss = 0.6931463479995728\n",
      "Batch 2: Loss = 0.6931358575820923\n",
      "Batch 3: Loss = 0.6931314468383789\n",
      "Batch 4: Loss = 0.6931201219558716\n",
      "Batch 5: Loss = 0.6931133270263672\n",
      "Batch 6: Loss = 0.6931107044219971\n",
      "Batch 7: Loss = 0.6931028366088867\n",
      "Batch 8: Loss = 0.6930952668190002\n",
      "Batch 9: Loss = 0.6930988430976868\n",
      "Batch 10: Loss = 0.693091094493866\n",
      "Batch 11: Loss = 0.6930738687515259\n",
      "Batch 12: Loss = 0.6930953860282898\n",
      "Batch 13: Loss = 0.693079948425293\n",
      "Batch 14: Loss = 0.6930753588676453\n",
      "Batch 15: Loss = 0.6930761933326721\n",
      "Batch 16: Loss = 0.6930736899375916\n",
      "Batch 17: Loss = 0.6930694580078125\n",
      "Batch 18: Loss = 0.6930606365203857\n",
      "Batch 19: Loss = 0.6930750012397766\n",
      "Batch 20: Loss = 0.6930649876594543\n",
      "Batch 21: Loss = 0.6930702328681946\n",
      "Batch 22: Loss = 0.6930771470069885\n",
      "Batch 23: Loss = 0.6930841207504272\n",
      "Batch 24: Loss = 0.6930593848228455\n",
      "Batch 25: Loss = 0.6930658221244812\n",
      "Batch 26: Loss = 0.6930628418922424\n",
      "Batch 27: Loss = 0.6930493712425232\n",
      "Batch 28: Loss = 0.6930767297744751\n",
      "Batch 29: Loss = 0.6930657625198364\n",
      "Batch 30: Loss = 0.6930657029151917\n",
      "Batch 31: Loss = 0.6930638551712036\n",
      "Batch 32: Loss = 0.6930773258209229\n",
      "Batch 33: Loss = 0.6930463314056396\n",
      "Batch 34: Loss = 0.6930423378944397\n",
      "Batch 35: Loss = 0.6930544972419739\n",
      "Batch 36: Loss = 0.6930341720581055\n",
      "Batch 37: Loss = 0.693061113357544\n",
      "Batch 38: Loss = 0.6930606961250305\n",
      "Batch 39: Loss = 0.6930536031723022\n",
      "Batch 40: Loss = 0.6930704712867737\n",
      "Batch 41: Loss = 0.6930384635925293\n",
      "Batch 42: Loss = 0.6930526494979858\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/littlemilk/MLG/HW1/DrBC.ipynb Cell 17\u001b[0m in \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnetai_940_2_littlemilk/home/littlemilk/MLG/HW1/DrBC.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=25'>26</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mvalidate\u001b[39m():\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnetai_940_2_littlemilk/home/littlemilk/MLG/HW1/DrBC.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bnetai_940_2_littlemilk/home/littlemilk/MLG/HW1/DrBC.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m _ \u001b[39m=\u001b[39m train()\n",
      "\u001b[1;32m/home/littlemilk/MLG/HW1/DrBC.ipynb Cell 17\u001b[0m in \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnetai_940_2_littlemilk/home/littlemilk/MLG/HW1/DrBC.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# optim\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnetai_940_2_littlemilk/home/littlemilk/MLG/HW1/DrBC.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=20'>21</a>\u001b[0m optim\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Bnetai_940_2_littlemilk/home/littlemilk/MLG/HW1/DrBC.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=21'>22</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnetai_940_2_littlemilk/home/littlemilk/MLG/HW1/DrBC.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=22'>23</a>\u001b[0m optim\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Bnetai_940_2_littlemilk/home/littlemilk/MLG/HW1/DrBC.ipynb#X22sdnNjb2RlLXJlbW90ZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mBatch \u001b[39m\u001b[39m{\u001b[39;00mi \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: Loss = \u001b[39m\u001b[39m{\u001b[39;00mloss\u001b[39m.\u001b[39mitem()\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/jupyterhub/lib/python3.9/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    490\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/jupyterhub/lib/python3.9/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    g_list, dg_list, bc_list  = prepare_synthetic()\n",
    "    print('-'*20, 'prepare systhetic done')\n",
    "    batch_cnt = len(g_list) // BATCH_SIZE\n",
    "\n",
    "    for i in range(batch_cnt):\n",
    "        s_index = i*BATCH_SIZE\n",
    "        e_index = (i+1)*BATCH_SIZE\n",
    "        train_g, train_dg, train_bc = g_list[s_index: e_index], dg_list[s_index: e_index], bc_list[s_index: e_index]\n",
    "        X, y, all_nb = preprocessing_data(train_g, train_dg, train_bc)\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        out = model(X, all_nb)\n",
    "\n",
    "        # pairwise-loss\n",
    "        s_ids, t_ids = get_pairwise_ids(train_g)\n",
    "        out_diff = out[s_ids] - out[t_ids]\n",
    "        y_diff = y[s_ids] - y[t_ids]\n",
    "        loss = loss_fn(out_diff, torch.sigmoid(y_diff))\n",
    "\n",
    "        # optim\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        print(f\"Batch {i + 1}: Loss = {loss.item()}\")\n",
    "        \n",
    "def validate():\n",
    "    pass\n",
    "\n",
    "_ = train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To-Do List\n",
    "* (done) loss_fn 再加上 sigmoid\n",
    "* (done) pairwise 目前跨圖了\n",
    "* (done) h 要 normalized\n",
    "* aggregate 改成 MessagePassing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyterhub",
   "language": "python",
   "name": "jupyterhub"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
