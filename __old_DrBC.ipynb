{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch_geometric\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import read_test1_data\n",
    "from utils import gen_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 11\n",
    "\n",
    "SYNTHETIC_NUM = 100\n",
    "# SYNTHETIC_NUM = 100\n",
    "\n",
    "# number of gen nodes\n",
    "NUM_MIN = 100\n",
    "NUM_MAX = 200\n",
    "\n",
    "LEARNING_RATE = 1e-4\n",
    "EMBEDDING_SIZE = 128\n",
    "DEPTH = 5\n",
    "BATCH_SIZE = 4\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test1_X, test1_bc = read_test1_data(0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Synthetic Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "423\n"
     ]
    }
   ],
   "source": [
    "train_g = gen_graph(NUM_MIN, NUM_MAX)\n",
    "print(len(train_g.edges()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 7, 8, 9, 15, 28, 33, 38, 41, 46, 48, 57, 69, 100]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(train_g.neighbors(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = []\n",
    "for node in list(train_g.nodes())[:5]:\n",
    "    ls.append(list(train_g.neighbors(node)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nx.betweenness_centrality(train_g)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DrBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Module, Linear, GRUCell, Sequential, ReLU, functional as t_F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_synthetic():\n",
    "    g_list = []\n",
    "    dg_list = []\n",
    "    bc_list = []\n",
    "    for i in range(SYNTHETIC_NUM):\n",
    "        g = gen_graph(NUM_MIN, NUM_MAX)\n",
    "        g_list.append(g)\n",
    "        dg_list.append(nx.degree(g))\n",
    "        bc_list.append(nx.betweenness_centrality(g))\n",
    "        \n",
    "    return g_list, dg_list, bc_list\n",
    "\n",
    "def preprocessing_data(train_g:list, train_dg:list, train_bc:list):\n",
    "    X = []\n",
    "    y = []\n",
    "    nb = []\n",
    "    pre_index = 0\n",
    "    for i in range(len(train_bc)):\n",
    "        assert len(train_dg[i]) == len(train_bc[i]) == len(train_g[i].nodes())\n",
    "        # make suer is has same nodes number.\n",
    "        num_node = len(train_dg[i])\n",
    "        for node_id in range(num_node):\n",
    "            node_nb = [pre_index+n for n in train_g[i].neighbors(node_id)]\n",
    "            X.append([train_dg[i][node_id], 1., 1.])\n",
    "            y.append(train_bc[i][node_id])\n",
    "            nb.append(node_nb)\n",
    "        pre_index += num_node\n",
    "    X = torch.Tensor(X)\n",
    "    y = torch.Tensor(y)\n",
    "    # print(X.shape, y.shape)\n",
    "\n",
    "    return X, y, nb\n",
    "\n",
    "def get_pairwise_ids(g_list):\n",
    "    s_ids = np.zeros(shape=(0, ), dtype=int)\n",
    "    t_ids = np.zeros(shape=(0, ), dtype=int)\n",
    "    pre_index = 0\n",
    "    for g in g_list:\n",
    "        num_node = len(g.nodes())\n",
    "        ids_1 = np.repeat(np.arange(pre_index, pre_index+num_node), 5)\n",
    "        ids_2 = np.repeat(np.arange(pre_index, pre_index+num_node), 5)\n",
    "\n",
    "        np.random.shuffle(ids_1)\n",
    "        np.random.shuffle(ids_2)\n",
    "\n",
    "        s_ids = np.append(s_ids, ids_1, axis=0)\n",
    "        t_ids = np.append(t_ids, ids_2, axis=0)\n",
    "        pre_index += num_node\n",
    "    return s_ids, t_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DrBC(Module):\n",
    "    def __init__(self, embedding_size=EMBEDDING_SIZE, depth=DEPTH):\n",
    "        super(DrBC, self).__init__()\n",
    "        self.embedding_size = embedding_size\n",
    "        self.depth = depth\n",
    "        self.linear0 = Linear(3, self.embedding_size)\n",
    "        self.gru = GRUCell(self.embedding_size, self.embedding_size)\n",
    "        # decoder\n",
    "        self.mlp = Sequential(\n",
    "            Linear(self.embedding_size, self.embedding_size // 2),\n",
    "            ReLU(),\n",
    "            Linear(self.embedding_size // 2, 1)\n",
    "        )\n",
    "        \n",
    "    def neighbor_aggre(self, X, all_nb, h):\n",
    "        # nb aggre\n",
    "        h_aggre = []\n",
    "        for node_id in range(X.shape[0]):\n",
    "            d_v = X[node_id, 0]\n",
    "            node_nb = all_nb[node_id]\n",
    "            node_aggre = torch.Tensor([0.] * self.embedding_size).to(device)\n",
    "            for nb_id in node_nb:\n",
    "                # for node all nb\n",
    "                node_aggre += (1 / (math.sqrt(d_v + 1) * math.sqrt(X[nb_id, 0] + 1))) * h[nb_id]\n",
    "            h_aggre.append(torch.unsqueeze(node_aggre, dim=0))\n",
    "        h_aggre = torch.cat(h_aggre, dim=0)\n",
    "        # print('h_aggre shape: ', h_aggre.shape)\n",
    "        return h_aggre # tensor format\n",
    "\n",
    "    def forward(self, X, all_nb):\n",
    "        all_h = []\n",
    "        h = self.linear0(X)\n",
    "        h = torch.relu(h)\n",
    "        h = t_F.normalize(h, p=2, dim=-1) # l2-norm\n",
    "        all_h.append(torch.unsqueeze(h, dim=0))\n",
    "\n",
    "        # GRUCell\n",
    "        for i in range(self.depth-1):\n",
    "            # neighborhood aggregation\n",
    "            h_aggre = self.neighbor_aggre(X, all_nb, h)\n",
    "            h = self.gru(h_aggre, h)\n",
    "            h = t_F.normalize(h, p=2, dim=-1) # l2-norm\n",
    "            all_h.append(torch.unsqueeze(h, dim=0))\n",
    "        \n",
    "        # max pooling\n",
    "        all_h = torch.cat(all_h, dim=0)\n",
    "        h_max = torch.max(all_h, dim=0).values\n",
    "        # print('h_max shape: ', h_max.shape)\n",
    "\n",
    "        # Decoder\n",
    "        out = self.mlp(h_max)\n",
    "        out = torch.squeeze(out)\n",
    "        # print('out shape: ', out.shape)\n",
    "        return out\n",
    "        \n",
    "\n",
    "\n",
    "        \n",
    "model = DrBC().to(device)\n",
    "optim = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.parameters of DrBC(\n",
       "  (linear0): Linear(in_features=3, out_features=128, bias=True)\n",
       "  (gru): GRUCell(128, 128)\n",
       "  (mlp): Sequential(\n",
       "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=64, out_features=1, bias=True)\n",
       "  )\n",
       ")>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- prepare systhetic done\n",
      "Batch 1: Loss = 0.6931397914886475\n",
      "Batch 2: Loss = 0.6931315660476685\n",
      "Batch 3: Loss = 0.6931244134902954\n",
      "Batch 4: Loss = 0.6931222081184387\n",
      "Batch 5: Loss = 0.6931186318397522\n",
      "Batch 6: Loss = 0.6931130290031433\n",
      "Batch 7: Loss = 0.6931073665618896\n",
      "Batch 8: Loss = 0.6931057572364807\n",
      "Batch 9: Loss = 0.6931012868881226\n",
      "Batch 10: Loss = 0.6930966377258301\n",
      "Batch 11: Loss = 0.6930989027023315\n",
      "Batch 12: Loss = 0.6930879354476929\n",
      "Batch 13: Loss = 0.6930891275405884\n",
      "Batch 14: Loss = 0.693083643913269\n",
      "Batch 15: Loss = 0.6930794715881348\n",
      "Batch 16: Loss = 0.6930931806564331\n",
      "Batch 17: Loss = 0.6930823922157288\n",
      "Batch 18: Loss = 0.6930931210517883\n",
      "Batch 19: Loss = 0.6930713057518005\n",
      "Batch 20: Loss = 0.6930682063102722\n",
      "Batch 21: Loss = 0.6930778622627258\n",
      "Batch 22: Loss = 0.6930816173553467\n",
      "Batch 23: Loss = 0.6930723190307617\n",
      "Batch 24: Loss = 0.6930813193321228\n",
      "Batch 25: Loss = 0.6930588483810425\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    g_list, dg_list, bc_list  = prepare_synthetic()\n",
    "    print('-'*20, 'prepare systhetic done')\n",
    "    batch_cnt = len(g_list) // BATCH_SIZE\n",
    "\n",
    "    for i in range(batch_cnt):\n",
    "        s_index = i*BATCH_SIZE\n",
    "        e_index = (i+1)*BATCH_SIZE\n",
    "        train_g, train_dg, train_bc = g_list[s_index: e_index], dg_list[s_index: e_index], bc_list[s_index: e_index]\n",
    "        X, y, all_nb = preprocessing_data(train_g, train_dg, train_bc)\n",
    "        X, y = X.to(device), y.to(device)\n",
    "        out = model(X, all_nb)\n",
    "\n",
    "        # pairwise-loss\n",
    "        s_ids, t_ids = get_pairwise_ids(train_g)\n",
    "        out_diff = out[s_ids] - out[t_ids]\n",
    "        y_diff = y[s_ids] - y[t_ids]\n",
    "        loss = loss_fn(out_diff, torch.sigmoid(y_diff))\n",
    "\n",
    "        # optim\n",
    "        optim.zero_grad()\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        print(f\"Batch {i + 1}: Loss = {loss.item()}\")\n",
    "        \n",
    "def validate():\n",
    "    pass\n",
    "\n",
    "_ = train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To-Do List\n",
    "* (done) loss_fn 再加上 sigmoid\n",
    "* (done) pairwise 目前跨圖了\n",
    "* (done) h 要 normalized\n",
    "* aggregate 改成 MessagePassing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyterhub",
   "language": "python",
   "name": "jupyterhub"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
